"""PolyBase external table DDL generator for Silver outputs.

Generates SQL Server PolyBase external table definitions and
views based on Silver metadata. Supports both STATE (SCD) and
EVENT entity types with appropriate query patterns.
"""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

if TYPE_CHECKING:
    from pipelines.lib.storage.base import StorageBackend

logger = logging.getLogger(__name__)

__all__ = [
    "PolyBaseConfig",
    "generate_external_table_ddl",
    "generate_polybase_setup",
    "generate_state_views",
    "generate_event_views",
    "generate_from_metadata",
    "generate_from_metadata_dict",
    "write_polybase_ddl_s3",
]


@dataclass
class PolyBaseConfig:
    """Configuration for PolyBase external table generation."""

    # External data source
    data_source_name: str
    data_source_location: str  # e.g., "wasbs://container@account.blob.core.windows.net/silver/"

    # External file format
    file_format_name: str = "parquet_format"
    format_type: str = "PARQUET"
    compression: str = "SNAPPY"

    # Schema and naming
    schema_name: str = "dbo"
    table_prefix: str = ""  # Optional prefix for table names

    # Credential (optional)
    credential_name: Optional[str] = None

    # S3 endpoint (for MinIO or custom S3-compatible storage)
    s3_endpoint: Optional[str] = None
    s3_access_key: Optional[str] = None  # Will be masked in output

    def external_table_name(self, entity_name: str, entity_kind: str) -> str:
        """Generate external table name for an entity."""
        suffix = "state" if entity_kind == "state" else "events"
        prefix = f"{self.table_prefix}_" if self.table_prefix else ""
        return f"{prefix}{entity_name}_{suffix}_external"


def generate_external_table_ddl(
    table_name: str,
    columns: List[Dict[str, Any]],
    location: str,
    config: PolyBaseConfig,
    *,
    partition_columns: Optional[List[str]] = None,
    description: str = "",
) -> str:
    """Generate CREATE EXTERNAL TABLE DDL.

    Args:
        table_name: Name for the external table
        columns: List of column dicts with name, sql_type, nullable
        location: Relative location within the data source
        config: PolyBase configuration
        partition_columns: Columns used for partitioning
        description: Optional description comment

    Returns:
        SQL DDL string for creating the external table
    """
    # Build column definitions
    column_defs = []
    for col in columns:
        col_name = col.get("name", "unnamed")
        col_type = col.get("sql_type", "NVARCHAR(4000)")
        nullable = " NULL" if col.get("nullable", True) else " NOT NULL"
        column_defs.append(f"    [{col_name}] {col_type}{nullable}")

    columns_sql = ",\n".join(column_defs)

    # Build partition info comment if applicable
    partition_comment = ""
    if partition_columns:
        partition_comment = f"\n-- Partitioned by: {', '.join(partition_columns)}"

    ddl = f"""-- {description or f'External Table for {table_name}'}
-- Generated by bronze-foundry pipelines{partition_comment}

IF OBJECT_ID('[{config.schema_name}].[{table_name}]', 'U') IS NOT NULL
    DROP EXTERNAL TABLE [{config.schema_name}].[{table_name}];

CREATE EXTERNAL TABLE [{config.schema_name}].[{table_name}]
(
{columns_sql}
)
WITH
(
    LOCATION = '{location}',
    DATA_SOURCE = [{config.data_source_name}],
    FILE_FORMAT = [{config.file_format_name}],
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
);
"""
    return ddl


def generate_data_source_ddl(config: PolyBaseConfig) -> str:
    """Generate CREATE EXTERNAL DATA SOURCE DDL.

    Args:
        config: PolyBase configuration

    Returns:
        SQL DDL string
    """
    credential_clause = ""
    if config.credential_name:
        credential_clause = f",\n        CREDENTIAL = [{config.credential_name}]"

    ddl = f"""-- External Data Source for Silver layer
IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = '{config.data_source_name}')
BEGIN
    CREATE EXTERNAL DATA SOURCE [{config.data_source_name}]
    WITH
    (
        TYPE = HADOOP,
        LOCATION = '{config.data_source_location}'{credential_clause}
    );
END
"""
    return ddl


def generate_file_format_ddl(config: PolyBaseConfig) -> str:
    """Generate CREATE EXTERNAL FILE FORMAT DDL.

    Args:
        config: PolyBase configuration

    Returns:
        SQL DDL string
    """
    ddl = f"""-- External File Format for Parquet
IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = '{config.file_format_name}')
BEGIN
    CREATE EXTERNAL FILE FORMAT [{config.file_format_name}]
    WITH
    (
        FORMAT_TYPE = {config.format_type},
        DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
    );
END
"""
    return ddl


def generate_polybase_setup(
    entity_name: str,
    columns: List[Dict[str, Any]],
    entity_kind: str,
    natural_keys: List[str],
    config: PolyBaseConfig,
    *,
    history_mode: str = "current_only",
    partition_columns: Optional[List[str]] = None,
    change_timestamp: str = "updated_at",
    delete_mode: str = "ignore",
    domain: Optional[str] = None,
    subject: Optional[str] = None,
    env_prefix: Optional[str] = None,
) -> str:
    """Generate complete PolyBase setup DDL for a Silver entity.

    Includes data source, file format, external table, and helper views.

    Args:
        entity_name: Name of the entity (used in SQL object names)
        columns: List of column dicts
        entity_kind: "state" or "event"
        natural_keys: Primary key columns
        config: PolyBase configuration
        history_mode: "current_only" or "full_history"
        partition_columns: Columns used for partitioning
        change_timestamp: Timestamp column name
        delete_mode: "ignore", "tombstone", or "hard_delete"
        domain: Business domain (used for Hive-style location path)
        subject: Subject area (used for Hive-style location path)
        env_prefix: Optional environment prefix (e.g., "production")

    Returns:
        Complete SQL setup script
    """
    parts = []

    # Header with credential setup instructions
    credential_setup = ""
    if config.credential_name:
        s3_endpoint_info = f"\n-- S3 Endpoint: {config.s3_endpoint}" if config.s3_endpoint else ""
        s3_access_info = f"\n-- Access Key: {config.s3_access_key}" if config.s3_access_key else ""
        credential_setup = f"""
-- ============================================
-- CREDENTIAL SETUP (run once per database)
-- ============================================
-- Before running this script, create the database scoped credential:
--
-- CREATE DATABASE SCOPED CREDENTIAL [{config.credential_name}]
-- WITH IDENTITY = '<access_key>',
-- SECRET = '<secret_key>';{s3_endpoint_info}{s3_access_info}
--
-- For MinIO/S3-compatible storage, you may also need to configure
-- the external data source with a custom endpoint.
-- ============================================
"""

    parts.append(f"""{credential_setup}-- ============================================
-- PolyBase Setup for: {entity_name}
-- Entity Kind: {entity_kind}
-- History Mode: {history_mode}
-- Generated by bronze-foundry pipelines
-- ============================================
""")

    # Data source and file format
    parts.append(generate_data_source_ddl(config))
    parts.append(generate_file_format_ddl(config))

    # External table
    table_name = config.external_table_name(entity_name, entity_kind)

    # Construct location path using Hive-style partitioning when domain/subject available
    if domain and subject:
        # Use Hive-style path: [env_prefix/]domain=X/subject=Y/
        if env_prefix:
            location = f"{env_prefix}/domain={domain}/subject={subject}/"
        else:
            location = f"domain={domain}/subject={subject}/"
    else:
        # Fallback to simple entity name path
        location = f"{entity_name}/"

    parts.append(generate_external_table_ddl(
        table_name,
        columns,
        location,
        config,
        partition_columns=partition_columns,
        description=f"Silver {entity_kind} entity: {entity_name}",
    ))

    # Generate appropriate views based on entity kind
    if entity_kind == "state":
        parts.append(generate_state_views(
            table_name,
            natural_keys,
            config,
            history_mode=history_mode,
            change_timestamp=change_timestamp,
            delete_mode=delete_mode,
        ))
    else:
        parts.append(generate_event_views(
            table_name,
            natural_keys,
            config,
            change_timestamp=change_timestamp,
            partition_columns=partition_columns,
        ))

    return "\n".join(parts)


def generate_state_views(
    external_table_name: str,
    natural_keys: Optional[List[str]],
    config: PolyBaseConfig,
    *,
    history_mode: str = "current_only",
    change_timestamp: str = "updated_at",
    delete_mode: str = "ignore",
) -> str:
    """Generate views for STATE (SCD) entities.

    Args:
        external_table_name: Name of the external table
        natural_keys: Primary key columns (None for periodic_snapshot without keys)
        config: PolyBase configuration
        history_mode: "current_only" or "full_history"
        change_timestamp: Timestamp column name
        delete_mode: "ignore", "tombstone", or "hard_delete"

    Returns:
        SQL DDL for views
    """
    schema = config.schema_name
    base_name = external_table_name.replace("_external", "")
    # Handle None natural_keys (periodic_snapshot without deduplication)
    natural_keys = natural_keys or []
    pk_cols = ", ".join([f"[{col}]" for col in natural_keys]) if natural_keys else ""

    # For tombstone mode, we need to filter out deleted records in views
    deleted_filter = ""
    if delete_mode == "tombstone":
        deleted_filter = " AND (_deleted = 0 OR _deleted IS NULL)"

    views = []

    # Current state view (always useful)
    if history_mode == "full_history":
        # For SCD2, filter by is_current flag
        current_view = f"""
-- Current State View (latest version of each entity)
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_current]
AS
SELECT *
FROM [{schema}].[{external_table_name}]
WHERE is_current = 1{deleted_filter};
"""
    else:
        # For SCD1, all records are current
        if delete_mode == "tombstone":
            current_view = f"""
-- Current State View (excludes soft-deleted records)
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_current]
AS
SELECT *
FROM [{schema}].[{external_table_name}]
WHERE _deleted = 0 OR _deleted IS NULL;
"""
        else:
            current_view = f"""
-- Current State View (all records are current in SCD1)
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_current]
AS
SELECT *
FROM [{schema}].[{external_table_name}];
"""
    views.append(current_view)

    # Point-in-time function for SCD2
    if history_mode == "full_history":
        pit_function = f"""
-- Point-in-Time Query Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_as_of]('2025-01-15')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_as_of]
(
    @as_of_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE effective_from <= @as_of_date
    AND (effective_to IS NULL OR effective_to > @as_of_date){deleted_filter};
"""
        views.append(pit_function)

        # History lookup function - only if natural_keys are defined
        if natural_keys:
            # History lookup function - supports single or composite keys
            if len(natural_keys) == 1:
                # Single key: simple parameter
                history_function = f"""
-- Entity History Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_history]('KEY_VALUE')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_history]
(
    @key_value NVARCHAR(255)
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [{natural_keys[0]}] = @key_value
    ORDER BY effective_from DESC;
"""
            else:
                # Composite key: multiple parameters
                params = ", ".join([f"@key_{i} NVARCHAR(255)" for i in range(len(natural_keys))])
                where_clauses = " AND ".join([f"[{key}] = @key_{i}" for i, key in enumerate(natural_keys)])
                param_list = ", ".join([f"@key_{i}" for i in range(len(natural_keys))])
                history_function = f"""
-- Entity History Function (Composite Key)
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_history]({param_list})
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_history]
(
    {params}
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE {where_clauses}
    ORDER BY effective_from DESC;
"""
            views.append(history_function)

            # History summary view - only if natural_keys are defined
            summary_view = f"""
-- History Summary View
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_history_summary]
AS
SELECT
    {pk_cols},
    COUNT(*) as version_count,
    MIN(effective_from) as first_version,
    MAX(effective_from) as latest_version,
    DATEDIFF(day, MIN(effective_from), MAX(effective_from)) as history_span_days
FROM [{schema}].[{external_table_name}]
GROUP BY {pk_cols};
"""
            views.append(summary_view)

    return "\n".join(views)


def generate_event_views(
    external_table_name: str,
    natural_keys: Optional[List[str]],
    config: PolyBaseConfig,
    *,
    change_timestamp: str = "event_ts",
    partition_columns: Optional[List[str]] = None,
) -> str:
    """Generate views for EVENT entities.

    Args:
        external_table_name: Name of the external table
        natural_keys: Primary key columns (None for entities without keys)
        config: PolyBase configuration
        change_timestamp: Event timestamp column
        partition_columns: Partition columns

    Returns:
        SQL DDL for views
    """
    schema = config.schema_name
    base_name = external_table_name.replace("_external", "")
    partition_col = partition_columns[0] if partition_columns else "event_date"
    # Handle None natural_keys
    natural_keys = natural_keys or []

    views = []

    # Date range function
    date_range_function = f"""
-- Date Range Query Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_for_dates]('2025-01-01', '2025-01-31')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_for_dates]
(
    @start_date DATE,
    @end_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [{partition_col}] >= @start_date
    AND [{partition_col}] <= @end_date;
"""
    views.append(date_range_function)

    # Single date function
    single_date_function = f"""
-- Single Date Query Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_for_date]('2025-01-15')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_for_date]
(
    @target_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [{partition_col}] = @target_date;
"""
    views.append(single_date_function)

    # Daily aggregation view - only if natural_keys are defined
    if natural_keys:
        daily_view = f"""
-- Daily Summary View
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_daily_summary]
AS
SELECT
    CAST([{change_timestamp}] AS DATE) as event_date,
    COUNT(*) as event_count,
    COUNT(DISTINCT [{natural_keys[0]}]) as unique_entities
FROM [{schema}].[{external_table_name}]
GROUP BY CAST([{change_timestamp}] AS DATE);
"""
        views.append(daily_view)
    else:
        # Simpler daily view without unique entity count
        daily_view = f"""
-- Daily Summary View
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_daily_summary]
AS
SELECT
    CAST([{change_timestamp}] AS DATE) as event_date,
    COUNT(*) as event_count
FROM [{schema}].[{external_table_name}]
GROUP BY CAST([{change_timestamp}] AS DATE);
"""
        views.append(daily_view)

    return "\n".join(views)


def generate_from_metadata(
    metadata_path: Path,
    config: PolyBaseConfig,
    *,
    entity_name: Optional[str] = None,
) -> str:
    """Generate PolyBase DDL from a _metadata.json file.

    Args:
        metadata_path: Path to _metadata.json file
        config: PolyBase configuration
        entity_name: Override entity name (defaults to directory name)

    Returns:
        Complete SQL setup script

    Example:
        >>> ddl = generate_from_metadata(
        ...     Path("./silver/orders/_metadata.json"),
        ...     PolyBaseConfig(
        ...         data_source_name="silver_source",
        ...         data_source_location="wasbs://silver@account.blob.core.windows.net/",
        ...     ),
        ... )
        >>> print(ddl)
    """
    # Load metadata
    with open(metadata_path, encoding="utf-8") as f:
        metadata = json.load(f)

    # Extract required fields
    columns = metadata.get("columns", [])
    entity_kind = metadata.get("entity_kind", "state")
    history_mode = metadata.get("history_mode", "current_only")
    natural_keys = metadata.get("natural_keys", [])
    change_timestamp = metadata.get("change_timestamp", "updated_at")
    partition_by = metadata.get("partition_by")
    delete_mode = metadata.get("delete_mode", "ignore")

    # Derive entity name from domain/subject if not explicitly provided
    # This avoids using path-derived names that may contain invalid chars like "dt=20250117"
    if not entity_name:
        domain = metadata.get("domain")
        subject = metadata.get("subject")
        if domain and subject:
            entity_name = f"{domain}_{subject}"
        elif subject:
            entity_name = subject
        else:
            # Fallback to parent directory name, but sanitize to remove invalid chars
            parent_name = metadata_path.parent.name
            # Remove characters that are invalid in SQL identifiers (like '=')
            entity_name = parent_name.replace("=", "_").replace("-", "_")

    # Generate full setup
    return generate_polybase_setup(
        entity_name,
        columns,
        entity_kind,
        natural_keys,
        config,
        history_mode=history_mode,
        partition_columns=partition_by,
        change_timestamp=change_timestamp,
        delete_mode=delete_mode,
    )


def write_polybase_script(
    output_path: Path,
    metadata_path: Path,
    config: PolyBaseConfig,
    *,
    entity_name: Optional[str] = None,
) -> Path:
    """Generate and write PolyBase DDL to a file.

    Args:
        output_path: Where to write the SQL script
        metadata_path: Path to _metadata.json file
        config: PolyBase configuration
        entity_name: Override entity name

    Returns:
        Path to the written script
    """
    ddl = generate_from_metadata(metadata_path, config, entity_name=entity_name)
    output_path.write_text(ddl, encoding="utf-8")
    logger.info("Wrote PolyBase script to %s", output_path)
    return output_path


def generate_from_metadata_dict(
    metadata: Dict[str, Any],
    config: PolyBaseConfig,
    *,
    entity_name: Optional[str] = None,
    env_prefix: Optional[str] = None,
) -> str:
    """Generate PolyBase DDL from metadata dictionary.

    This function accepts an in-memory metadata dict rather than reading
    from a file, making it suitable for cloud storage scenarios.

    Args:
        metadata: Metadata dictionary (same structure as _metadata.json)
        config: PolyBase configuration
        entity_name: Entity name (optional, derived from domain/subject if not provided)
        env_prefix: Optional environment prefix for location path (e.g., "production")

    Returns:
        Complete SQL setup script

    Example:
        >>> metadata = {"columns": [...], "entity_kind": "state", "domain": "sales", "subject": "orders"}
        >>> ddl = generate_from_metadata_dict(
        ...     metadata,
        ...     PolyBaseConfig(
        ...         data_source_name="silver_source",
        ...         data_source_location="s3://bucket/silver/",
        ...     ),
        ... )  # entity_name derived as "sales_orders"
    """
    # Extract required fields
    columns = metadata.get("columns", [])
    entity_kind = metadata.get("entity_kind", "state")
    history_mode = metadata.get("history_mode", "current_only")
    natural_keys = metadata.get("natural_keys", [])
    change_timestamp = metadata.get("change_timestamp", "updated_at")
    partition_by = metadata.get("partition_by")
    delete_mode = metadata.get("delete_mode", "ignore")
    domain = metadata.get("domain")
    subject = metadata.get("subject")

    # Derive entity name from domain/subject if not explicitly provided
    if not entity_name:
        if domain and subject:
            entity_name = f"{domain}_{subject}"
        elif subject:
            entity_name = subject
        else:
            raise ValueError(
                "entity_name is required when metadata lacks domain/subject fields"
            )

    # Generate full setup
    return generate_polybase_setup(
        entity_name,
        columns,
        entity_kind,
        natural_keys,
        config,
        history_mode=history_mode,
        partition_columns=partition_by,
        change_timestamp=change_timestamp,
        delete_mode=delete_mode,
        domain=domain,
        subject=subject,
        env_prefix=env_prefix,
    )


def write_polybase_ddl_s3(
    storage: "StorageBackend",
    metadata: Dict[str, Any],
    config: PolyBaseConfig,
    *,
    entity_name: str,
    env_prefix: Optional[str] = None,
    filename: str = "_polybase.sql",
) -> bool:
    """Write PolyBase DDL script to S3/cloud storage.

    Generates and writes a _polybase.sql file alongside the data and metadata
    in cloud storage. This makes it easy to set up SQL Server PolyBase
    external tables pointing to the Silver layer data.

    Args:
        storage: Storage backend to write to (S3, ADLS, etc.)
        metadata: Metadata dictionary with columns, entity_kind, etc.
        config: PolyBase configuration with data source details
        entity_name: Name of the entity (used in table/view names)
        env_prefix: Optional environment prefix for location path (e.g., "production")
        filename: Output filename (default: _polybase.sql)

    Returns:
        True if DDL was written successfully

    Example:
        >>> from pipelines.lib.storage import get_storage
        >>> from pipelines.lib.polybase import PolyBaseConfig, write_polybase_ddl_s3
        >>>
        >>> storage = get_storage("s3://bucket/silver/orders/")
        >>> config = PolyBaseConfig(
        ...     data_source_name="silver_minio",
        ...     data_source_location="s3://mdf/silver/",
        ...     credential_name="minio_credential",
        ... )
        >>> write_polybase_ddl_s3(storage, metadata, config, entity_name="orders")
    """
    try:
        ddl = generate_from_metadata_dict(
            metadata, config, entity_name=entity_name, env_prefix=env_prefix
        )
        result = storage.write_text(filename, ddl)
        if result.success:
            logger.info(
                "Wrote PolyBase DDL to S3: %s/%s",
                storage.base_path,
                filename,
            )
            return True
        else:
            logger.warning("Failed to write PolyBase DDL: %s", result.error)
            return False
    except Exception as e:
        logger.warning("Error writing PolyBase DDL to S3: %s", e)
        return False
