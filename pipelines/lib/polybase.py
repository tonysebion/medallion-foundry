"""PolyBase external table DDL generator for Silver outputs.

Generates SQL Server PolyBase external table definitions and
views based on Silver metadata. Supports both STATE (SCD) and
EVENT entity types with appropriate query patterns.
"""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)

__all__ = [
    "PolyBaseConfig",
    "generate_external_table_ddl",
    "generate_polybase_setup",
    "generate_state_views",
    "generate_event_views",
    "generate_from_metadata",
]


@dataclass
class PolyBaseConfig:
    """Configuration for PolyBase external table generation."""

    # External data source
    data_source_name: str
    data_source_location: str  # e.g., "wasbs://container@account.blob.core.windows.net/silver/"

    # External file format
    file_format_name: str = "parquet_format"
    format_type: str = "PARQUET"
    compression: str = "SNAPPY"

    # Schema and naming
    schema_name: str = "dbo"
    table_prefix: str = ""  # Optional prefix for table names

    # Credential (optional)
    credential_name: Optional[str] = None

    def external_table_name(self, entity_name: str, entity_kind: str) -> str:
        """Generate external table name for an entity."""
        suffix = "state" if entity_kind == "state" else "events"
        prefix = f"{self.table_prefix}_" if self.table_prefix else ""
        return f"{prefix}{entity_name}_{suffix}_external"


def generate_external_table_ddl(
    table_name: str,
    columns: List[Dict[str, Any]],
    location: str,
    config: PolyBaseConfig,
    *,
    partition_columns: Optional[List[str]] = None,
    description: str = "",
) -> str:
    """Generate CREATE EXTERNAL TABLE DDL.

    Args:
        table_name: Name for the external table
        columns: List of column dicts with name, sql_type, nullable
        location: Relative location within the data source
        config: PolyBase configuration
        partition_columns: Columns used for partitioning
        description: Optional description comment

    Returns:
        SQL DDL string for creating the external table
    """
    # Build column definitions
    column_defs = []
    for col in columns:
        col_name = col.get("name", "unnamed")
        col_type = col.get("sql_type", "NVARCHAR(4000)")
        nullable = " NULL" if col.get("nullable", True) else " NOT NULL"
        column_defs.append(f"    [{col_name}] {col_type}{nullable}")

    columns_sql = ",\n".join(column_defs)

    # Build partition info comment if applicable
    partition_comment = ""
    if partition_columns:
        partition_comment = f"\n-- Partitioned by: {', '.join(partition_columns)}"

    ddl = f"""-- {description or f'External Table for {table_name}'}
-- Generated by bronze-foundry pipelines{partition_comment}

IF OBJECT_ID('[{config.schema_name}].[{table_name}]', 'U') IS NOT NULL
    DROP EXTERNAL TABLE [{config.schema_name}].[{table_name}];

CREATE EXTERNAL TABLE [{config.schema_name}].[{table_name}]
(
{columns_sql}
)
WITH
(
    LOCATION = '{location}',
    DATA_SOURCE = [{config.data_source_name}],
    FILE_FORMAT = [{config.file_format_name}],
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
);
"""
    return ddl


def generate_data_source_ddl(config: PolyBaseConfig) -> str:
    """Generate CREATE EXTERNAL DATA SOURCE DDL.

    Args:
        config: PolyBase configuration

    Returns:
        SQL DDL string
    """
    credential_clause = ""
    if config.credential_name:
        credential_clause = f",\n    CREDENTIAL = [{config.credential_name}]"

    ddl = f"""-- External Data Source for Silver layer
IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = '{config.data_source_name}')
BEGIN
    CREATE EXTERNAL DATA SOURCE [{config.data_source_name}]
    WITH
    (
        TYPE = HADOOP,
        LOCATION = '{config.data_source_location}'{credential_clause}
    );
END
"""
    return ddl


def generate_file_format_ddl(config: PolyBaseConfig) -> str:
    """Generate CREATE EXTERNAL FILE FORMAT DDL.

    Args:
        config: PolyBase configuration

    Returns:
        SQL DDL string
    """
    ddl = f"""-- External File Format for Parquet
IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = '{config.file_format_name}')
BEGIN
    CREATE EXTERNAL FILE FORMAT [{config.file_format_name}]
    WITH
    (
        FORMAT_TYPE = {config.format_type},
        DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
    );
END
"""
    return ddl


def generate_polybase_setup(
    entity_name: str,
    columns: List[Dict[str, Any]],
    entity_kind: str,
    natural_keys: List[str],
    config: PolyBaseConfig,
    *,
    history_mode: str = "current_only",
    partition_columns: Optional[List[str]] = None,
    change_timestamp: str = "updated_at",
) -> str:
    """Generate complete PolyBase setup DDL for a Silver entity.

    Includes data source, file format, external table, and helper views.

    Args:
        entity_name: Name of the entity
        columns: List of column dicts
        entity_kind: "state" or "event"
        natural_keys: Primary key columns
        config: PolyBase configuration
        history_mode: "current_only" or "full_history"
        partition_columns: Columns used for partitioning
        change_timestamp: Timestamp column name

    Returns:
        Complete SQL setup script
    """
    parts = []

    # Header
    parts.append(f"""-- ============================================
-- PolyBase Setup for: {entity_name}
-- Entity Kind: {entity_kind}
-- History Mode: {history_mode}
-- Generated by bronze-foundry pipelines
-- ============================================
""")

    # Data source and file format
    parts.append(generate_data_source_ddl(config))
    parts.append(generate_file_format_ddl(config))

    # External table
    table_name = config.external_table_name(entity_name, entity_kind)
    location = f"{entity_name}/"

    parts.append(generate_external_table_ddl(
        table_name,
        columns,
        location,
        config,
        partition_columns=partition_columns,
        description=f"Silver {entity_kind} entity: {entity_name}",
    ))

    # Generate appropriate views based on entity kind
    if entity_kind == "state":
        parts.append(generate_state_views(
            table_name,
            natural_keys,
            config,
            history_mode=history_mode,
            change_timestamp=change_timestamp,
        ))
    else:
        parts.append(generate_event_views(
            table_name,
            natural_keys,
            config,
            change_timestamp=change_timestamp,
            partition_columns=partition_columns,
        ))

    return "\n".join(parts)


def generate_state_views(
    external_table_name: str,
    natural_keys: List[str],
    config: PolyBaseConfig,
    *,
    history_mode: str = "current_only",
    change_timestamp: str = "updated_at",
) -> str:
    """Generate views for STATE (SCD) entities.

    Args:
        external_table_name: Name of the external table
        natural_keys: Primary key columns
        config: PolyBase configuration
        history_mode: "current_only" or "full_history"
        change_timestamp: Timestamp column name

    Returns:
        SQL DDL for views
    """
    schema = config.schema_name
    base_name = external_table_name.replace("_external", "")
    pk_cols = ", ".join([f"[{col}]" for col in natural_keys])

    views = []

    # Current state view (always useful)
    if history_mode == "full_history":
        # For SCD2, filter by is_current flag
        current_view = f"""
-- Current State View (latest version of each entity)
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_current]
AS
SELECT *
FROM [{schema}].[{external_table_name}]
WHERE is_current = 1;
"""
    else:
        # For SCD1, all records are current
        current_view = f"""
-- Current State View (all records are current in SCD1)
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_current]
AS
SELECT *
FROM [{schema}].[{external_table_name}];
"""
    views.append(current_view)

    # Point-in-time function for SCD2
    if history_mode == "full_history":
        pit_function = f"""
-- Point-in-Time Query Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_as_of]('2025-01-15')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_as_of]
(
    @as_of_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE effective_from <= @as_of_date
    AND (effective_to IS NULL OR effective_to > @as_of_date);
"""
        views.append(pit_function)

        # History lookup function
        history_function = f"""
-- Entity History Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_history]('KEY_VALUE')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_history]
(
    @key_value NVARCHAR(255)
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [{natural_keys[0]}] = @key_value
    ORDER BY effective_from DESC;
"""
        views.append(history_function)

        # History summary view
        summary_view = f"""
-- History Summary View
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_history_summary]
AS
SELECT
    {pk_cols},
    COUNT(*) as version_count,
    MIN(effective_from) as first_version,
    MAX(effective_from) as latest_version,
    DATEDIFF(day, MIN(effective_from), MAX(effective_from)) as history_span_days
FROM [{schema}].[{external_table_name}]
GROUP BY {pk_cols};
"""
        views.append(summary_view)

    return "\n".join(views)


def generate_event_views(
    external_table_name: str,
    natural_keys: List[str],
    config: PolyBaseConfig,
    *,
    change_timestamp: str = "event_ts",
    partition_columns: Optional[List[str]] = None,
) -> str:
    """Generate views for EVENT entities.

    Args:
        external_table_name: Name of the external table
        natural_keys: Primary key columns
        config: PolyBase configuration
        change_timestamp: Event timestamp column
        partition_columns: Partition columns

    Returns:
        SQL DDL for views
    """
    schema = config.schema_name
    base_name = external_table_name.replace("_external", "")
    partition_col = partition_columns[0] if partition_columns else "event_date"

    views = []

    # Date range function
    date_range_function = f"""
-- Date Range Query Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_for_dates]('2025-01-01', '2025-01-31')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_for_dates]
(
    @start_date DATE,
    @end_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [{partition_col}] >= @start_date
    AND [{partition_col}] <= @end_date;
"""
    views.append(date_range_function)

    # Single date function
    single_date_function = f"""
-- Single Date Query Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_for_date]('2025-01-15')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_for_date]
(
    @target_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [{partition_col}] = @target_date;
"""
    views.append(single_date_function)

    # Daily aggregation view
    daily_view = f"""
-- Daily Summary View
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_daily_summary]
AS
SELECT
    CAST([{change_timestamp}] AS DATE) as event_date,
    COUNT(*) as event_count,
    COUNT(DISTINCT [{natural_keys[0]}]) as unique_entities
FROM [{schema}].[{external_table_name}]
GROUP BY CAST([{change_timestamp}] AS DATE);
"""
    views.append(daily_view)

    return "\n".join(views)


def generate_from_metadata(
    metadata_path: Path,
    config: PolyBaseConfig,
    *,
    entity_name: Optional[str] = None,
) -> str:
    """Generate PolyBase DDL from a _metadata.json file.

    Args:
        metadata_path: Path to _metadata.json file
        config: PolyBase configuration
        entity_name: Override entity name (defaults to directory name)

    Returns:
        Complete SQL setup script

    Example:
        >>> ddl = generate_from_metadata(
        ...     Path("./silver/orders/_metadata.json"),
        ...     PolyBaseConfig(
        ...         data_source_name="silver_source",
        ...         data_source_location="wasbs://silver@account.blob.core.windows.net/",
        ...     ),
        ... )
        >>> print(ddl)
    """
    # Load metadata
    with open(metadata_path, encoding="utf-8") as f:
        metadata = json.load(f)

    # Derive entity name if not provided
    if not entity_name:
        entity_name = metadata_path.parent.name

    # Extract required fields
    columns = metadata.get("columns", [])
    entity_kind = metadata.get("entity_kind", "state")
    history_mode = metadata.get("history_mode", "current_only")
    natural_keys = metadata.get("natural_keys", [])
    change_timestamp = metadata.get("change_timestamp", "updated_at")
    partition_by = metadata.get("partition_by")

    # Generate full setup
    return generate_polybase_setup(
        entity_name,
        columns,
        entity_kind,
        natural_keys,
        config,
        history_mode=history_mode,
        partition_columns=partition_by,
        change_timestamp=change_timestamp,
    )


def write_polybase_script(
    output_path: Path,
    metadata_path: Path,
    config: PolyBaseConfig,
    *,
    entity_name: Optional[str] = None,
) -> Path:
    """Generate and write PolyBase DDL to a file.

    Args:
        output_path: Where to write the SQL script
        metadata_path: Path to _metadata.json file
        config: PolyBase configuration
        entity_name: Override entity name

    Returns:
        Path to the written script
    """
    ddl = generate_from_metadata(metadata_path, config, entity_name=entity_name)
    output_path.write_text(ddl, encoding="utf-8")
    logger.info("Wrote PolyBase script to %s", output_path)
    return output_path
