"""PolyBase external table DDL generator for Silver outputs.

Generates SQL Server PolyBase external table definitions and
views based on Silver metadata. Supports both STATE (SCD) and
EVENT entity types with appropriate query patterns.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

from pipelines.lib.observability import get_structlog_logger

if TYPE_CHECKING:
    from pipelines.lib.storage.base import StorageBackend

logger = get_structlog_logger(__name__)

__all__ = [
    "PolyBaseConfig",
    "generate_external_table_ddl",
    "generate_polybase_setup",
    "generate_state_views",
    "generate_event_views",
    "generate_from_metadata",
    "generate_from_metadata_dict",
    "write_polybase_ddl_s3",
    "write_polybase_artifacts",
]


@dataclass
class PolyBaseConfig:
    """Configuration for PolyBase external table generation."""

    # External data source
    data_source_name: str
    data_source_location: (
        str  # e.g., "wasbs://container@account.blob.core.windows.net/silver/"
    )

    # External file format
    file_format_name: str = "parquet_format"
    format_type: str = "PARQUET"
    compression: str = "SNAPPY"

    # Schema and naming
    schema_name: str = "dbo"
    table_prefix: str = ""  # Optional prefix for table names

    # Credential (optional)
    credential_name: Optional[str] = None

    # S3 endpoint (for MinIO or custom S3-compatible storage)
    s3_endpoint: Optional[str] = None
    s3_access_key: Optional[str] = None  # Will be masked in output

    def external_table_name(self, entity_name: str, entity_kind: str) -> str:
        """Generate external table name for an entity."""
        suffix = "state" if entity_kind == "state" else "events"
        prefix = f"{self.table_prefix}_" if self.table_prefix else ""
        return f"{prefix}{entity_name}_{suffix}_external"


def _validate_column_references(
    columns: List[Dict[str, Any]],
    unique_columns: Optional[List[str]],
    last_updated_column: Optional[str],
    history_mode: str,
    delete_mode: str,
) -> List[str]:
    """Validate that all columns referenced in DDL views exist in metadata.

    Returns list of missing column descriptions (empty if all columns exist).
    """
    column_names = {col["name"] for col in columns}
    missing = []

    # Check unique columns
    for key in unique_columns or []:
        if key not in column_names:
            missing.append(f"unique_column '{key}'")

    # Check last_updated_column
    if last_updated_column and last_updated_column not in column_names:
        missing.append(f"last_updated_column '{last_updated_column}'")

    # Check SCD2 temporal columns
    if history_mode == "full_history":
        for col in ["effective_from", "effective_to", "is_current"]:
            if col not in column_names:
                missing.append(f"SCD2 column '{col}'")

    # Check tombstone column
    if delete_mode == "tombstone" and "_deleted" not in column_names:
        missing.append("'_deleted' (tombstone mode)")

    return missing


def generate_external_table_ddl(
    table_name: str,
    columns: List[Dict[str, Any]],
    location: str,
    config: PolyBaseConfig,
    *,
    partition_columns: Optional[List[str]] = None,
    description: str = "",
    include_dt_partition: bool = False,
) -> str:
    """Generate CREATE EXTERNAL TABLE DDL.

    Args:
        table_name: Name for the external table
        columns: List of column dicts with name, sql_type, nullable
        location: Relative location within the data source
        config: PolyBase configuration
        partition_columns: Columns used for partitioning
        description: Optional description comment
        include_dt_partition: If True, adds dt DATE column for Hive partition pruning

    Returns:
        SQL DDL string for creating the external table
    """
    # Build column definitions
    column_defs = []
    for col in columns:
        col_name = col.get("name", "unnamed")
        col_type = col.get("sql_type", "NVARCHAR(4000)")
        nullable = " NULL" if col.get("nullable", True) else " NOT NULL"
        column_defs.append(f"    [{col_name}] {col_type}{nullable}")

    # Add dt partition column for Hive-style partition pruning
    # Only add if dt is not already in the column definitions
    if include_dt_partition:
        existing_names = {col.get("name") for col in columns}
        if "dt" not in existing_names:
            column_defs.append(
                "    [dt] DATE NULL  -- Hive partition column (inferred from folder path)"
            )

    columns_sql = ",\n".join(column_defs)

    # Build partition info comment if applicable
    partition_comment = ""
    if partition_columns:
        partition_comment = f"\n-- Partitioned by: {', '.join(partition_columns)}"

    ddl = f"""-- {description or f"External Table for {table_name}"}
-- Generated by bronze-foundry pipelines{partition_comment}

IF OBJECT_ID('[{config.schema_name}].[{table_name}]', 'U') IS NOT NULL
    DROP EXTERNAL TABLE [{config.schema_name}].[{table_name}];

CREATE EXTERNAL TABLE [{config.schema_name}].[{table_name}]
(
{columns_sql}
)
WITH
(
    LOCATION = '{location}',
    DATA_SOURCE = [{config.data_source_name}],
    FILE_FORMAT = [{config.file_format_name}],
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
);
"""
    return ddl


def generate_data_source_ddl(config: PolyBaseConfig) -> str:
    """Generate CREATE EXTERNAL DATA SOURCE DDL.

    Args:
        config: PolyBase configuration

    Returns:
        SQL DDL string
    """
    credential_clause = ""
    if config.credential_name:
        credential_clause = f",\n        CREDENTIAL = [{config.credential_name}]"

    ddl = f"""-- External Data Source for Silver layer
IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = '{config.data_source_name}')
BEGIN
    CREATE EXTERNAL DATA SOURCE [{config.data_source_name}]
    WITH
    (
        TYPE = HADOOP,
        LOCATION = '{config.data_source_location}'{credential_clause}
    );
END
"""
    return ddl


def generate_file_format_ddl(config: PolyBaseConfig) -> str:
    """Generate CREATE EXTERNAL FILE FORMAT DDL.

    Args:
        config: PolyBase configuration

    Returns:
        SQL DDL string
    """
    ddl = f"""-- External File Format for Parquet
IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = '{config.file_format_name}')
BEGIN
    CREATE EXTERNAL FILE FORMAT [{config.file_format_name}]
    WITH
    (
        FORMAT_TYPE = {config.format_type},
        DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
    );
END
"""
    return ddl


def generate_polybase_setup(
    entity_name: str,
    columns: List[Dict[str, Any]],
    entity_kind: str,
    unique_columns: List[str],
    config: PolyBaseConfig,
    *,
    history_mode: str = "current_only",
    partition_columns: Optional[List[str]] = None,
    last_updated_column: str = "updated_at",
    delete_mode: str = "ignore",
    domain: Optional[str] = None,
    subject: Optional[str] = None,
    env_prefix: Optional[str] = None,
    include_dt_partition: bool = True,
) -> str:
    """Generate complete PolyBase setup DDL for a Silver entity.

    Includes data source, file format, external table, and helper views.

    Args:
        entity_name: Name of the entity (used in SQL object names)
        columns: List of column dicts
        entity_kind: "state" or "event"
        unique_columns: Primary key columns
        config: PolyBase configuration
        history_mode: "current_only" or "full_history"
        partition_columns: Columns used for partitioning
        last_updated_column: Timestamp column name
        delete_mode: "ignore", "tombstone", or "hard_delete"
        domain: Business domain (used for Hive-style location path)
        subject: Subject area (used for Hive-style location path)
        env_prefix: Optional environment prefix (e.g., "production")
        include_dt_partition: If True, adds dt DATE column for Hive partition pruning

    Returns:
        Complete SQL setup script
    """
    parts = []

    # Validate that all referenced columns exist in metadata
    missing_cols = _validate_column_references(
        columns, unique_columns, last_updated_column, history_mode, delete_mode
    )
    if missing_cols:
        missing_list = "\n".join(f"  - {col}" for col in missing_cols)
        parts.append(f"""/*
WARNING: The following columns are referenced in views but not found in metadata:
{missing_list}
Views referencing these columns may fail at runtime.
*/
""")

    # Header with credential setup instructions
    credential_setup = ""
    if config.credential_name:
        s3_endpoint_info = (
            f"\n-- S3 Endpoint: {config.s3_endpoint}" if config.s3_endpoint else ""
        )
        s3_access_info = (
            f"\n-- Access Key: {config.s3_access_key}" if config.s3_access_key else ""
        )
        credential_setup = f"""
-- ============================================
-- CREDENTIAL SETUP (run once per database)
-- ============================================
-- Before running this script, create the database scoped credential:
--
-- CREATE DATABASE SCOPED CREDENTIAL [{config.credential_name}]
-- WITH IDENTITY = '<access_key>',
-- SECRET = '<secret_key>';{s3_endpoint_info}{s3_access_info}
--
-- For MinIO/S3-compatible storage, you may also need to configure
-- the external data source with a custom endpoint.
-- ============================================
"""

    # Use domain as schema name when available (more natural organization)
    # This creates: [domain].[subject_state_external] instead of [dbo].[domain_subject_state_external]
    if domain:
        effective_schema = domain
        base_table_name = subject or entity_name
    else:
        effective_schema = config.schema_name
        base_table_name = entity_name

    # Create a modified config with the effective schema for view generation
    effective_config = PolyBaseConfig(
        data_source_name=config.data_source_name,
        data_source_location=config.data_source_location,
        file_format_name=config.file_format_name,
        format_type=config.format_type,
        compression=config.compression,
        schema_name=effective_schema,
        table_prefix=config.table_prefix,
        credential_name=config.credential_name,
        s3_endpoint=config.s3_endpoint,
        s3_access_key=config.s3_access_key,
    )

    parts.append(f"""{credential_setup}-- ============================================
-- PolyBase Setup for: {entity_name}
-- Entity Kind: {entity_kind}
-- History Mode: {history_mode}
-- Schema: {effective_schema} (domain-based)
-- Generated by bronze-foundry pipelines
-- ============================================
""")

    # Schema creation DDL (ensure schema exists)
    if domain:
        schema_ddl = f"""
-- Create schema for domain (if not exists)
IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = '{effective_schema}')
BEGIN
    EXEC('CREATE SCHEMA [{effective_schema}]');
END
"""
        parts.append(schema_ddl)

    # Data source and file format
    parts.append(generate_data_source_ddl(config))
    parts.append(generate_file_format_ddl(config))

    # External table - use subject as base name when domain is available
    table_name = effective_config.external_table_name(base_table_name, entity_kind)

    # Construct location path using Hive-style partitioning when domain/subject available
    if domain and subject:
        # Use Hive-style path: [env_prefix/]domain=X/subject=Y/
        if env_prefix:
            location = f"{env_prefix}/domain={domain}/subject={subject}/"
        else:
            location = f"domain={domain}/subject={subject}/"
    else:
        # Fallback to simple entity name path
        location = f"{entity_name}/"

    parts.append(
        generate_external_table_ddl(
            table_name,
            columns,
            location,
            effective_config,
            partition_columns=partition_columns,
            description=f"Silver {entity_kind} entity: {entity_name}",
            include_dt_partition=include_dt_partition,
        )
    )

    # Generate appropriate views based on entity kind
    if entity_kind == "state":
        parts.append(
            generate_state_views(
                table_name,
                unique_columns,
                effective_config,
                history_mode=history_mode,
                last_updated_column=last_updated_column,
                delete_mode=delete_mode,
                include_dt_views=include_dt_partition,
            )
        )
    else:
        parts.append(
            generate_event_views(
                table_name,
                unique_columns,
                effective_config,
                last_updated_column=last_updated_column,
                partition_columns=partition_columns,
                use_dt_partition=include_dt_partition,
            )
        )

    return "\n".join(parts)


def generate_state_views(
    external_table_name: str,
    unique_columns: Optional[List[str]],
    config: PolyBaseConfig,
    *,
    history_mode: str = "current_only",
    last_updated_column: str = "updated_at",
    delete_mode: str = "ignore",
    include_dt_views: bool = True,
) -> str:
    """Generate views for STATE (SCD) entities.

    Args:
        external_table_name: Name of the external table
        unique_columns: Primary key columns (None for periodic_snapshot without keys)
        config: PolyBase configuration
        history_mode: "current_only" or "full_history"
        last_updated_column: Timestamp column name
        delete_mode: "ignore", "tombstone", or "hard_delete"
        include_dt_views: If True, adds date-filtered views using dt partition column

    Returns:
        SQL DDL for views
    """
    schema = config.schema_name
    base_name = external_table_name.replace("_external", "")
    # Handle None unique_columns (periodic_snapshot without deduplication)
    unique_columns = unique_columns or []
    pk_cols = (
        ", ".join([f"[{col}]" for col in unique_columns]) if unique_columns else ""
    )

    # For tombstone mode, we need to filter out deleted records in views
    deleted_filter = ""
    if delete_mode == "tombstone":
        deleted_filter = " AND (_deleted = 0 OR _deleted IS NULL)"

    views = []

    # Current state view (always useful)
    if history_mode == "full_history":
        # For SCD2, filter by is_current flag
        current_view = f"""
-- Current State View (latest version of each entity)
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_current]
AS
SELECT *
FROM [{schema}].[{external_table_name}]
WHERE is_current = 1{deleted_filter};
"""
    else:
        # For SCD1, all records are current
        if delete_mode == "tombstone":
            current_view = f"""
-- Current State View (excludes soft-deleted records)
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_current]
AS
SELECT *
FROM [{schema}].[{external_table_name}]
WHERE _deleted = 0 OR _deleted IS NULL;
"""
        else:
            current_view = f"""
-- Current State View (all records are current in SCD1)
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_current]
AS
SELECT *
FROM [{schema}].[{external_table_name}];
"""
    views.append(current_view)

    # Point-in-time function for SCD2
    if history_mode == "full_history":
        pit_function = f"""
-- Point-in-Time Query Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_as_of]('2025-01-15')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_as_of]
(
    @as_of_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE effective_from <= @as_of_date
    AND (effective_to IS NULL OR effective_to > @as_of_date){deleted_filter};
"""
        views.append(pit_function)

        # History lookup function - only if unique_columns are defined
        if unique_columns:
            # History lookup function - supports single or composite keys
            if len(unique_columns) == 1:
                # Single key: simple parameter
                history_function = f"""
-- Entity History Function
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_history]('KEY_VALUE')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_history]
(
    @key_value NVARCHAR(255)
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [{unique_columns[0]}] = @key_value
    ORDER BY effective_from DESC;
"""
            else:
                # Composite key: multiple parameters
                params = ", ".join(
                    [f"@key_{i} NVARCHAR(255)" for i in range(len(unique_columns))]
                )
                where_clauses = " AND ".join(
                    [f"[{key}] = @key_{i}" for i, key in enumerate(unique_columns)]
                )
                param_list = ", ".join(
                    [f"@key_{i}" for i in range(len(unique_columns))]
                )
                history_function = f"""
-- Entity History Function (Composite Key)
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_history]({param_list})
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_history]
(
    {params}
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE {where_clauses}
    ORDER BY effective_from DESC;
"""
            views.append(history_function)

            # History summary view - only if unique_columns are defined
            summary_view = f"""
-- History Summary View
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_history_summary]
AS
SELECT
    {pk_cols},
    COUNT(*) as version_count,
    MIN(effective_from) as first_version,
    MAX(effective_from) as latest_version,
    DATEDIFF(day, MIN(effective_from), MAX(effective_from)) as history_span_days
FROM [{schema}].[{external_table_name}]
GROUP BY {pk_cols};
"""
            views.append(summary_view)

    # Date-filtered views for partition pruning (using dt column)
    if include_dt_views:
        date_range_function = f"""
-- Date Range Query Function (uses dt partition for efficient scanning)
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_for_dates]('2025-01-01', '2025-01-31')
-- Note: Filters on [dt] column enable PolyBase partition pruning
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_for_dates]
(
    @start_date DATE,
    @end_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [dt] >= @start_date
    AND [dt] <= @end_date{deleted_filter};
"""
        views.append(date_range_function)

        single_date_function = f"""
-- Single Date Query Function (uses dt partition)
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_for_date]('2025-01-15')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_for_date]
(
    @target_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [dt] = @target_date{deleted_filter};
"""
        views.append(single_date_function)

    return "\n".join(views)


def generate_event_views(
    external_table_name: str,
    unique_columns: Optional[List[str]],
    config: PolyBaseConfig,
    *,
    last_updated_column: str = "event_ts",
    partition_columns: Optional[List[str]] = None,
    use_dt_partition: bool = True,
) -> str:
    """Generate views for EVENT entities.

    Args:
        external_table_name: Name of the external table
        unique_columns: Primary key columns (None for entities without keys)
        config: PolyBase configuration
        last_updated_column: Event timestamp column
        partition_columns: Partition columns
        use_dt_partition: If True, uses dt column for partition-aware queries

    Returns:
        SQL DDL for views
    """
    schema = config.schema_name
    base_name = external_table_name.replace("_external", "")
    # Use dt column for partition-aware queries when available
    if use_dt_partition:
        partition_col = "dt"
    else:
        partition_col = partition_columns[0] if partition_columns else "event_date"
    # Handle None unique_columns
    unique_columns = unique_columns or []

    views = []

    # Date range function
    date_range_function = f"""
-- Date Range Query Function (uses dt partition for efficient scanning)
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_for_dates]('2025-01-01', '2025-01-31')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_for_dates]
(
    @start_date DATE,
    @end_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [{partition_col}] >= @start_date
    AND [{partition_col}] <= @end_date;
"""
    views.append(date_range_function)

    # Single date function
    single_date_function = f"""
-- Single Date Query Function (uses dt partition)
-- Usage: SELECT * FROM [{schema}].[fn_{base_name}_for_date]('2025-01-15')
CREATE OR ALTER FUNCTION [{schema}].[fn_{base_name}_for_date]
(
    @target_date DATE
)
RETURNS TABLE
AS
RETURN
    SELECT *
    FROM [{schema}].[{external_table_name}]
    WHERE [{partition_col}] = @target_date;
"""
    views.append(single_date_function)

    # Daily aggregation view - only if unique_columns are defined
    if unique_columns:
        daily_view = f"""
-- Daily Summary View
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_daily_summary]
AS
SELECT
    CAST([{last_updated_column}] AS DATE) as event_date,
    COUNT(*) as event_count,
    COUNT(DISTINCT [{unique_columns[0]}]) as unique_entities
FROM [{schema}].[{external_table_name}]
GROUP BY CAST([{last_updated_column}] AS DATE);
"""
        views.append(daily_view)
    else:
        # Simpler daily view without unique entity count
        daily_view = f"""
-- Daily Summary View
CREATE OR ALTER VIEW [{schema}].[vw_{base_name}_daily_summary]
AS
SELECT
    CAST([{last_updated_column}] AS DATE) as event_date,
    COUNT(*) as event_count
FROM [{schema}].[{external_table_name}]
GROUP BY CAST([{last_updated_column}] AS DATE);
"""
        views.append(daily_view)

    return "\n".join(views)


def generate_from_metadata(
    metadata_path: Path,
    config: PolyBaseConfig,
    *,
    entity_name: Optional[str] = None,
    include_dt_partition: bool = True,
) -> str:
    """Generate PolyBase DDL from a _metadata.json file.

    Args:
        metadata_path: Path to _metadata.json file
        config: PolyBase configuration
        entity_name: Override entity name (defaults to directory name)
        include_dt_partition: If True, adds dt DATE column for Hive partition pruning

    Returns:
        Complete SQL setup script

    Example:
        >>> ddl = generate_from_metadata(
        ...     Path("./silver/orders/_metadata.json"),
        ...     PolyBaseConfig(
        ...         data_source_name="silver_source",
        ...         data_source_location="wasbs://silver@account.blob.core.windows.net/",
        ...     ),
        ... )
        >>> print(ddl)
    """
    # Load metadata
    with open(metadata_path, encoding="utf-8") as f:
        metadata = json.load(f)

    # Extract required fields
    columns = metadata.get("columns", [])
    entity_kind = metadata.get("entity_kind", "state")
    history_mode = metadata.get("history_mode", "current_only")
    unique_columns = metadata.get("unique_columns", [])
    # Handle None explicitly - use "or" so explicit None uses default
    last_updated_column = metadata.get("last_updated_column") or "updated_at"
    partition_by = metadata.get("partition_by")
    delete_mode = metadata.get("delete_mode", "ignore")

    # Derive entity name from domain/subject if not explicitly provided
    # This avoids using path-derived names that may contain invalid chars like "dt=20250117"
    if not entity_name:
        domain = metadata.get("domain")
        subject = metadata.get("subject")
        if domain and subject:
            entity_name = f"{domain}_{subject}"
        elif subject:
            entity_name = subject
        else:
            # Fallback to parent directory name, but sanitize to remove invalid chars
            parent_name = metadata_path.parent.name
            # Remove characters that are invalid in SQL identifiers (like '=')
            entity_name = parent_name.replace("=", "_").replace("-", "_")

    # Generate full setup
    return generate_polybase_setup(
        entity_name,
        columns,
        entity_kind,
        unique_columns,
        config,
        history_mode=history_mode,
        partition_columns=partition_by,
        last_updated_column=last_updated_column,
        delete_mode=delete_mode,
        include_dt_partition=include_dt_partition,
    )


def write_polybase_script(
    output_path: Path,
    metadata_path: Path,
    config: PolyBaseConfig,
    *,
    entity_name: Optional[str] = None,
) -> Path:
    """Generate and write PolyBase DDL to a file.

    Args:
        output_path: Where to write the SQL script
        metadata_path: Path to _metadata.json file
        config: PolyBase configuration
        entity_name: Override entity name

    Returns:
        Path to the written script
    """
    ddl = generate_from_metadata(metadata_path, config, entity_name=entity_name)
    output_path.write_text(ddl, encoding="utf-8")
    logger.info("Wrote PolyBase script to %s", output_path)
    return output_path


def generate_from_metadata_dict(
    metadata: Dict[str, Any],
    config: PolyBaseConfig,
    *,
    entity_name: Optional[str] = None,
    env_prefix: Optional[str] = None,
    include_dt_partition: bool = True,
) -> str:
    """Generate PolyBase DDL from metadata dictionary.

    This function accepts an in-memory metadata dict rather than reading
    from a file, making it suitable for cloud storage scenarios.

    Args:
        metadata: Metadata dictionary (same structure as _metadata.json)
        config: PolyBase configuration
        entity_name: Entity name (optional, derived from domain/subject if not provided)
        env_prefix: Optional environment prefix for location path (e.g., "production")
        include_dt_partition: If True, adds dt DATE column for Hive partition pruning

    Returns:
        Complete SQL setup script

    Example:
        >>> metadata = {"columns": [...], "entity_kind": "state", "domain": "sales", "subject": "orders"}
        >>> ddl = generate_from_metadata_dict(
        ...     metadata,
        ...     PolyBaseConfig(
        ...         data_source_name="silver_source",
        ...         data_source_location="s3://bucket/silver/",
        ...     ),
        ... )  # entity_name derived as "sales_orders"
    """
    # Extract required fields
    columns = metadata.get("columns", [])
    entity_kind = metadata.get("entity_kind", "state")
    history_mode = metadata.get("history_mode", "current_only")
    unique_columns = metadata.get("unique_columns", [])
    # Handle None explicitly - use "or" so explicit None uses default
    last_updated_column = metadata.get("last_updated_column") or "updated_at"
    partition_by = metadata.get("partition_by")
    delete_mode = metadata.get("delete_mode", "ignore")
    domain = metadata.get("domain")
    subject = metadata.get("subject")

    # Derive entity name from domain/subject if not explicitly provided
    if not entity_name:
        if domain and subject:
            entity_name = f"{domain}_{subject}"
        elif subject:
            entity_name = subject
        else:
            raise ValueError(
                "entity_name is required when metadata lacks domain/subject fields. "
                "Either set 'domain' and 'subject' in your Silver YAML config, "
                "or pass entity_name explicitly to generate_from_metadata_dict()."
            )

    # Generate full setup
    return generate_polybase_setup(
        entity_name,
        columns,
        entity_kind,
        unique_columns,
        config,
        history_mode=history_mode,
        partition_columns=partition_by,
        last_updated_column=last_updated_column,
        delete_mode=delete_mode,
        domain=domain,
        subject=subject,
        env_prefix=env_prefix,
        include_dt_partition=include_dt_partition,
    )


def write_polybase_ddl_s3(
    storage: "StorageBackend",
    metadata: Dict[str, Any],
    config: PolyBaseConfig,
    *,
    entity_name: str,
    env_prefix: Optional[str] = None,
    filename: str = "_polybase.sql",
) -> bool:
    """Write PolyBase DDL script to S3/cloud storage.

    Generates and writes a _polybase.sql file alongside the data and metadata
    in cloud storage. This makes it easy to set up SQL Server PolyBase
    external tables pointing to the Silver layer data.

    Args:
        storage: Storage backend to write to (S3, ADLS, etc.)
        metadata: Metadata dictionary with columns, entity_kind, etc.
        config: PolyBase configuration with data source details
        entity_name: Name of the entity (used in table/view names)
        env_prefix: Optional environment prefix for location path (e.g., "production")
        filename: Output filename (default: _polybase.sql)

    Returns:
        True if DDL was written successfully

    Example:
        >>> from pipelines.lib.storage import get_storage
        >>> from pipelines.lib.polybase import PolyBaseConfig, write_polybase_ddl_s3
        >>>
        >>> storage = get_storage("s3://bucket/silver/orders/")
        >>> config = PolyBaseConfig(
        ...     data_source_name="silver_minio",
        ...     data_source_location="s3://mdf/silver/",
        ...     credential_name="minio_credential",
        ... )
        >>> write_polybase_ddl_s3(storage, metadata, config, entity_name="orders")
    """
    try:
        ddl = generate_from_metadata_dict(
            metadata, config, entity_name=entity_name, env_prefix=env_prefix
        )
        result = storage.write_text(filename, ddl)
        if result.success:
            logger.info(
                "Wrote PolyBase DDL to S3: %s/%s",
                storage.base_path,
                filename,
            )
            return True
        else:
            logger.warning("Failed to write PolyBase DDL: %s", result.error)
            return False
    except Exception as e:
        logger.warning("Error writing PolyBase DDL to S3: %s", e)
        return False


def write_polybase_artifacts(
    target: str,
    columns: List[Dict[str, Any]],
    *,
    domain: Optional[str] = None,
    subject: Optional[str] = None,
    entity_kind: str = "state",
    history_mode: str = "current_only",
    delete_mode: str = "ignore",
    unique_columns: Optional[List[str]] = None,
    last_updated_column: Optional[str] = None,
    storage_options: Optional[Dict[str, Any]] = None,
) -> Optional[str]:
    """Generate and write PolyBase DDL artifacts to cloud storage.

    This is a high-level convenience function that handles the complete workflow:
    1. Parses target path to extract bucket/container and environment prefix
    2. Derives entity name from domain/subject
    3. Creates PolyBaseConfig with appropriate settings
    4. Writes the DDL script to cloud storage

    Args:
        target: Target path (s3:// or abfss://)
        columns: List of column dicts with name, sql_type, nullable
        domain: Business domain (optional)
        subject: Subject area (optional)
        entity_kind: "state" or "event"
        history_mode: "current_only" or "full_history"
        delete_mode: "ignore", "tombstone", or "hard_delete"
        unique_columns: Primary key columns
        last_updated_column: Timestamp column name
        storage_options: Storage backend options (credentials, endpoint, etc.)

    Returns:
        Filename of the DDL script if written successfully ("_polybase.sql"), None on failure

    Example:
        >>> path = write_polybase_artifacts(
        ...     "s3://bucket/silver/domain=sales/subject=orders/",
        ...     columns=[{"name": "id", "sql_type": "INT"}],
        ...     domain="sales",
        ...     subject="orders",
        ... )
    """
    import os
    import re

    from pipelines.lib._path_utils import is_s3_path, parse_s3_uri
    from pipelines.lib.storage import get_storage

    try:
        storage_opts = storage_options or {}
        storage = get_storage(target, **storage_opts)

        # Derive entity name from domain/subject (preferred) or fall back to inference
        if domain and subject:
            entity_name = f"{domain}_{subject}"
        elif subject:
            entity_name = subject
        else:
            # Infer from path - extract last meaningful segment
            parts = target.rstrip("/").split("/")
            entity_name = parts[-1] if parts else "unknown"
            # Sanitize for SQL identifiers
            entity_name = entity_name.replace("=", "_").replace("-", "_")

        # Extract optional environment prefix between /silver/ and /domain=
        # e.g., "s3://bucket/silver/production/domain=..." -> "production"
        env_match = re.search(r"/silver/([^/]+)/domain=", target)
        env_prefix = env_match.group(1) if env_match else None

        # Extract bucket/container from S3 URI
        if is_s3_path(target):
            bucket, _ = parse_s3_uri(target)
            data_source_location = f"s3://{bucket}/silver/"
            data_source_name = f"silver_{bucket}"
        else:
            data_source_location = target.rsplit("/silver/", 1)[0] + "/silver/"
            data_source_name = "silver_adls"

        polybase_metadata = {
            "columns": columns,
            "entity_kind": entity_kind,
            "history_mode": history_mode,
            "delete_mode": delete_mode,
            "unique_columns": unique_columns or [],
            "last_updated_column": last_updated_column,
            "domain": domain,
            "subject": subject,
        }

        s3_endpoint = os.environ.get("AWS_ENDPOINT_URL", "https://s3.amazonaws.com")
        s3_access_key = os.environ.get("AWS_ACCESS_KEY_ID", "<your_access_key>")

        polybase_config = PolyBaseConfig(
            data_source_name=data_source_name,
            data_source_location=data_source_location,
            credential_name="s3_credential",
            s3_endpoint=s3_endpoint,
            s3_access_key=s3_access_key,
        )

        success = write_polybase_ddl_s3(
            storage,
            polybase_metadata,
            polybase_config,
            entity_name=entity_name,
            env_prefix=env_prefix,
        )
        if success:
            logger.debug("polybase_artifacts_written", target=target)
            return "_polybase.sql"
        return None
    except Exception as e:
        logger.warning("polybase_artifacts_failed", error=str(e))
        return None
