# yaml-language-server: $schema=../schema/pipeline.schema.json
#
# PIPELINE TEMPLATE: Copy this file and fill in the blanks
# =========================================================
#
# To run:
#   python -m pipelines ./your_pipeline.yaml --date 2025-01-15
#
# To test:
#   python -m pipelines ./your_pipeline.yaml --date 2025-01-15 --dry-run
#
# To validate:
#   python -m pipelines ./your_pipeline.yaml --date 2025-01-15 --check
#
# FILE LOCATION & NAMING
# ----------------------
# Place YAML files anywhere - just reference the full path when running.
# Convention: pipelines/{system}_{entity}.yaml
#
# SECRETS & ENVIRONMENT VARIABLES
# -------------------------------
# NEVER hardcode passwords, API keys, or tokens in pipeline files.
# Use ${VAR_NAME} syntax - values are expanded at runtime:
#
#     host: ${DB_HOST}
#     password: ${DB_PASSWORD}
#
# Set variables before running:
#     # Linux/Mac
#     export DB_HOST="server.example.com"
#     export DB_PASSWORD="secret"
#
#     # Windows PowerShell
#     $env:DB_HOST = "server.example.com"
#     $env:DB_PASSWORD = "secret"
#
#     # Windows Command Prompt
#     set DB_HOST=server.example.com
#     set DB_PASSWORD=secret
#
# Pipeline fails with clear error if required variables are missing.

# Optional metadata
name: your_pipeline_name
description: Brief description of what this pipeline does

# ============================================================
# LOGGING: Configure structured logging (optional)
# ============================================================
# Uncomment the logging section to override environment defaults
# logging:
#   # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
#   level: INFO
#
#   # Output format:
#   # - 'console': Human-readable (good for development)
#   # - 'json': Structured JSON (for Splunk and log aggregation)
#   format: console
#
#   # Optional: Write logs to a file (in addition to console)
#   # file: ./logs/pipeline.log
#
#   # Output to console? (default: true)
#   console: true
#
#   # Include timestamp in logs? (default: true)
#   include_timestamp: true
#
#   # Include source file:line info? (default: true for dev, false for prod)
#   include_source: true

# ============================================================
# BRONZE: Where does the data come from?
# ============================================================
bronze:
  # SYSTEM: The source system name (e.g., "salesforce", "erp", "legacy_db")
  system: your_system

  # ENTITY: The table or file name (e.g., "customers", "orders", "products")
  entity: your_entity

  # SOURCE TYPE: What kind of source is this?
  # Options: file_csv, file_parquet, file_json, file_jsonl, file_excel,
  #          file_fixed_width, database_mssql, database_postgres,
  #          database_mysql, database_db2, api_rest
  source_type: file_csv

  # SOURCE PATH: Where to find the data
  # For files: path to the file (use {run_date} for date-based files)
  # For databases: leave empty (use connection info below)
  source_path: "./data/your_file_{run_date}.csv"

  # DATABASE CONNECTION (only for database sources):
  # host: ${DB_HOST}           # Database host (use env var)
  # database: YourDatabase     # Database name
  # query: SELECT * FROM table # Optional custom SQL

  # LOAD PATTERN: How to load the data?
  # Options: full_snapshot (replace all each run)
  #          incremental (only new records)
  #          cdc (change data capture)
  load_pattern: full_snapshot

  # WATERMARK: For incremental loads, which column tracks changes?
  # watermark_column: updated_at

  # PERIODIC FULL REFRESH: Force full reload every N days (optional)
  # full_refresh_days: 7

  # CHUNKING: For large datasets, process in chunks (optional)
  # chunk_size: 100000

  # ============================================================
  # STORAGE OUTPUT: Where to write Bronze data (optional)
  # ============================================================
  # If omitted, auto-generates: ./bronze/system=your_system/entity=your_entity/dt={run_date}/
  # Only {run_date} is expanded - system/entity values are used literally

  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # AWS S3 Configuration:
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # target_path: "s3://my-bucket/bronze/your_system/your_entity/dt={run_date}/"
  #
  # Set these environment variables before running:
  #   export AWS_ACCESS_KEY_ID="your_access_key"
  #   export AWS_SECRET_ACCESS_KEY="your_secret_key"
  #   export AWS_REGION="us-east-1"

  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # S3-Compatible Storage (MinIO, Nutanix Objects, Ceph, etc.):
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # target_path: "s3://my-bucket/bronze/your_system/your_entity/dt={run_date}/"
  # options:
  #   s3_signature_version: s3v4    # Required for most S3-compatible storage
  #   s3_addressing_style: path     # Required for most S3-compatible storage
  #
  # Set these environment variables before running:
  #   export AWS_ACCESS_KEY_ID="your_access_key"
  #   export AWS_SECRET_ACCESS_KEY="your_secret_key"
  #   export AWS_ENDPOINT_URL="https://minio.example.com:9000"  # Your S3-compatible endpoint

  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # Azure Data Lake Storage (ADLS) Configuration:
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # target_path: "abfss://container@account.dfs.core.windows.net/bronze/your_system/your_entity/"
  #
  # Option 1: Storage account key
  #   export AZURE_STORAGE_ACCOUNT="your_account"
  #   export AZURE_STORAGE_KEY="your_key"
  #
  # Option 2: Service principal
  #   export AZURE_CLIENT_ID="your_client_id"
  #   export AZURE_CLIENT_SECRET="your_secret"
  #   export AZURE_TENANT_ID="your_tenant_id"

# ============================================================
# SILVER: How should data be curated?
# ============================================================
silver:
  # DOMAIN: Business domain this data belongs to
  # Examples: "sales", "finance", "hr", "operations"
  domain: your_domain

  # SUBJECT: Subject area within the domain
  # Examples: "orders", "customers", "employees", "transactions"
  subject: your_subject

  # NATURAL KEYS: What makes a record unique? (the primary key)
  # Examples: [id], [customer_id], [order_id, line_id]
  natural_keys: [id]

  # CHANGE TIMESTAMP: Which column has the "last updated" time?
  # This is used to detect changes and order records
  change_timestamp: updated_at

  # ENTITY KIND: What type of data is this?
  # state = Dimension (customer, product, account) - can change over time
  # event = Fact/Log (orders, clicks, payments) - immutable
  entity_kind: state

  # HISTORY MODE: How to handle changes over time?
  # current_only (or scd1) = Only keep latest version
  # full_history (or scd2) = Keep all versions with effective dates
  history_mode: current_only

  # OPTIONAL: Specify which columns to include (default: all)
  # attributes:
  #   - name
  #   - email
  #   - status

  # OPTIONAL: Exclude these columns (cannot use with attributes)
  # exclude_columns:
  #   - internal_id
  #   - password_hash

  # ============================================================
  # STORAGE OUTPUT: Where to write Silver data (optional)
  # ============================================================
  # If omitted, auto-generates: ./silver/domain={domain}/subject={subject}/dt={run_date}/

  # AWS S3:
  # target_path: "s3://my-bucket/silver/domain={domain}/subject={subject}/dt={run_date}/"

  # S3-Compatible (MinIO, Nutanix):
  # target_path: "s3://my-bucket/silver/domain={domain}/subject={subject}/dt={run_date}/"
  # (Same options and env vars as Bronze section above)

  # Azure Data Lake Storage (ADLS):
  # target_path: "abfss://container@account.dfs.core.windows.net/silver/domain={domain}/subject={subject}/dt={run_date}/"
