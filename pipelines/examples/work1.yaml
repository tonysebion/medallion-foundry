# yaml-language-server: $schema=../schema/pipeline.schema.json
# ============================================================================
# PIPELINE: CSV File to S3 Storage (MinIO/Nutanix Objects)
# ============================================================================
#
# WHAT THIS DOES:
#   1. Reads a CSV file from your local filesystem
#   2. Writes raw data to S3 "bronze" bucket (unchanged, with metadata)
#   3. Curates data to S3 "silver" bucket (deduplicated, timestamped)
#
# OUTPUT FILES CREATED:
#   Bronze: s3://bronze-bucket/system=CRM/entity=education/dt=2025-01-15/
#     - data.parquet       (your raw data converted to Parquet)
#     - _metadata.json     (schema, source info, lineage)
#     - _checksums.json    (SHA256 hashes for data integrity)
#
#   Silver: s3://silver-bucket/CRM/education/
#     - data.parquet       (curated, deduplicated data)
#     - _metadata.json     (schema for PolyBase DDL generation)
#     - _checksums.json    (SHA256 hashes for data integrity)
#     - _polybase.sql      (SQL Server external table DDL - ready to use!)
#
# ============================================================================
# BEFORE YOU RUN - Set these 3 environment variables:
# ============================================================================
#
#   Windows (Command Prompt):
#     set S3_ENDPOINT_URL=https://your-minio-server:9000
#     set AWS_ACCESS_KEY_ID=your-access-key
#     set AWS_SECRET_ACCESS_KEY=your-secret-key
#
#   Windows (PowerShell):
#     $env:S3_ENDPOINT_URL = "https://your-minio-server:9000"
#     $env:AWS_ACCESS_KEY_ID = "your-access-key"
#     $env:AWS_SECRET_ACCESS_KEY = "your-secret-key"
#
#   Linux/Mac:
#     export S3_ENDPOINT_URL=https://your-minio-server:9000
#     export AWS_ACCESS_KEY_ID=your-access-key
#     export AWS_SECRET_ACCESS_KEY=your-secret-key
#
# ============================================================================
# TO RUN:
# ============================================================================
#
#   python -m pipelines ./pipelines/examples/work1.yaml --date 2025-01-15
#
#   Options:
#     --dry-run    Validate configuration without writing data
#     --check      Pre-flight check (verify source exists, S3 is reachable)
#     --explain    Show what the pipeline will do without running it
#
# ============================================================================

# Optional: Give your pipeline a name and description
name: csv_to_s3_minio
description: Load CSV files to S3-compatible storage (MinIO, Nutanix Objects)

# ============================================================================
# BRONZE LAYER - Raw Data Extraction
# ============================================================================
# The Bronze layer lands data exactly as received from the source.
# It adds technical metadata but does NOT transform the data.

bronze:
  # SYSTEM: Logical grouping for your data source
  #   Examples: "CRM", "ERP", "Salesforce", "legacy_mainframe"
  #   This becomes part of the S3 path: s3://bucket/system=CRM/...
  system: CRM

  # ENTITY: What this data represents (like a table name)
  #   Examples: "customers", "orders", "products", "employees"
  #   This becomes part of the S3 path: s3://bucket/.../entity=education/...
  entity: education

  # SOURCE_TYPE: How to read the input file
  #   Options:
  #     file_csv          - Comma-separated values (most common)
  #     file_parquet      - Apache Parquet columnar format
  #     file_fixed_width  - Fixed-width positional files (mainframe)
  #     file_json         - JSON files
  #     file_jsonl        - JSON Lines (one JSON object per line)
  #     file_excel        - Excel spreadsheets (.xlsx, .xls)
  #     database_mssql    - SQL Server database
  #     api_rest          - REST API endpoint
  source_type: file_csv

  # SOURCE_PATH: Where to read data from
  #   Placeholders you can use:
  #     {run_date}  - The date you pass with --date (e.g., 2025-01-15)
  #     {entity}    - The entity name from above
  #     {system}    - The system name from above
  #
  #   Examples:
  #     ./data/orders_{run_date}.csv           -> ./data/orders_2025-01-15.csv
  #     ./data/{system}/{entity}_{run_date}.csv -> ./data/CRM/education_2025-01-15.csv
  source_path: "./pipelines/examples/data/{entity}_{run_date}.csv"

  # TARGET_PATH: Where to write Bronze output (S3 bucket)
  #   The path uses Hive-style partitioning: system=X/entity=Y/dt=Z
  #   This makes it easy to query with tools like Spark, Presto, or DuckDB
  target_path: "s3://bronze-bucket/system={system}/entity={entity}/dt={run_date}/"

  # LOAD_PATTERN: How to load data from source
  #   Options:
  #     full_snapshot  - Load entire dataset each run (most common, safest)
  #     incremental    - Only load new/changed records (requires watermark_column)
  #     cdc            - Change Data Capture (insert/update/delete markers)
  load_pattern: full_snapshot

  # ============================================================================
  # S3-COMPATIBLE STORAGE CONFIGURATION
  # ============================================================================
  # These settings are required for MinIO, Nutanix Objects, or any S3-compatible storage.
  # For actual AWS S3, you typically only need the credentials (no endpoint_url needed).

  # S3_ENDPOINT_URL: Your S3-compatible server address
  #   Use ${VAR_NAME} syntax to read from environment variables (recommended for security)
  #   Examples:
  #     ${S3_ENDPOINT_URL}              - Read from environment variable
  #     https://minio.mycompany.com:9000 - Direct URL (not recommended - use env vars)
  #     http://localhost:9000            - Local MinIO for development
  s3_endpoint_url: ${S3_ENDPOINT_URL}

  # S3_SIGNATURE_VERSION: Authentication signature version
  #   Use "s3v4" for all modern S3-compatible storage (MinIO, Nutanix, etc.)
  #   Only use "s3" for very old S3-compatible systems
  s3_signature_version: s3v4

  # S3_ADDRESSING_STYLE: How to format S3 URLs
  #   "path"    - Use path-style URLs: https://endpoint/bucket/key (required for MinIO)
  #   "virtual" - Use virtual-hosted style: https://bucket.endpoint/key (AWS default)
  #   "auto"    - Let the SDK decide
  s3_addressing_style: path

  # S3_VERIFY_SSL: Verify SSL certificates?
  #   false - Skip SSL verification (required for self-signed certificates)
  #   true  - Verify SSL certificates (use for production with valid certs)
  #   Default is false because most MinIO/local setups use self-signed certs
  s3_verify_ssl: false

  # OPTIONS: Additional configuration
  #   Credentials are passed here using environment variable syntax
  options:
    # AWS_ACCESS_KEY_ID: Your S3 access key (like a username)
    key: ${AWS_ACCESS_KEY_ID}

    # AWS_SECRET_ACCESS_KEY: Your S3 secret key (like a password)
    secret: ${AWS_SECRET_ACCESS_KEY}

# ============================================================================
# SILVER LAYER - Data Curation
# ============================================================================
# The Silver layer transforms raw Bronze data into a clean, queryable format.
# It handles deduplication, type casting, and maintains change history.
#
# NOTE: This Silver section is SELF-CONTAINED. It includes all S3 settings
# explicitly so it can run independently from a separate file if needed.
# In a combined pipeline, Silver auto-wires settings from Bronze.

silver:
  # DOMAIN: Business domain this data belongs to
  # Examples: "sales", "finance", "hr", "operations"
  domain: crm

  # SUBJECT: Subject area within the domain
  # Examples: "orders", "customers", "employees", "transactions"
  subject: education

  # NATURAL_KEYS: Column(s) that uniquely identify a record
  #   This is like a primary key in a database.
  #   Single key:   unique_columns: [customer_id]
  #   Composite:    unique_columns: [order_id, line_number]
  unique_columns: [id]

  # CHANGE_TIMESTAMP: Column containing when the record was last modified
  #   This is used to determine which version of a record is newest.
  #   Common names: updated_at, modified_date, LastModified, _timestamp
  last_updated_column: updated_at

  # SOURCE_PATH: Where to read Bronze data from
  #   This points to the Bronze output location.
  #   When running Bronze+Silver together, this is auto-wired.
  #   When running Silver separately, you must specify this explicitly.
  source_path: "s3://bronze-bucket/system=CRM/entity=education/dt={run_date}/"

  # TARGET_PATH: Where to write Silver output
  target_path: "s3://silver-bucket/domain=crm/subject=education/"

  # MODEL: Pre-built transformation pattern (simplifies configuration)
  #   Options for standard data:
  #     periodic_snapshot  - Simple dimension refresh, no deduplication
  #     full_merge_dedupe  - Deduplicated current view (SCD Type 1) - requires incremental Bronze
  #     incremental_merge  - For incremental loads with change tracking
  #     scd_type_2         - Full history with effective dates (is_current flag)
  #     event_log          - Immutable event stream (no deduplication)
  #
  #   Options for CDC (Change Data Capture) data:
  #     cdc_current           - CDC to current state (deletes ignored)
  #     cdc_current_tombstone - CDC with soft deletes (_deleted flag)
  #     cdc_current_hard_delete - CDC, deleted records removed
  #     cdc_history           - CDC with full history (SCD2)
  #     cdc_history_tombstone - CDC with full history and soft deletes
  #     cdc_history_hard_delete - CDC with full history, deletes removed
  #
  # For full_snapshot Bronze, use periodic_snapshot (simple dimension refresh).
  # For incremental Bronze with watermark, use full_merge_dedupe or scd_type_2.
  model: periodic_snapshot

  # VALIDATE_SOURCE: Should Silver verify Bronze checksums before processing?
  #   "skip"   - No validation (fastest, default)
  #   "warn"   - Log warning if checksums fail, but continue
  #   "strict" - Fail if checksums don't match
  validate_source: skip

  # ============================================================================
  # SILVER S3 CONFIGURATION (Self-contained for independent execution)
  # ============================================================================
  # These are repeated from Bronze so Silver can run independently.
  # When running the full pipeline, these are auto-wired from Bronze.

  s3_endpoint_url: ${S3_ENDPOINT_URL}
  s3_signature_version: s3v4
  s3_addressing_style: path
  s3_verify_ssl: false
