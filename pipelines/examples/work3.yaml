# yaml-language-server: $schema=../schema/pipeline.schema.json
# ============================================================================
# PIPELINE: Parent-Child Fixed-Width File (Multi-Record Type)
# ============================================================================
#
# WHAT THIS DOES:
#   Reads a fixed-width file where lines come in ABABBB patterns:
#   - "A" lines = Parent/Master records (e.g., Customer)
#   - "B" lines = Child/Detail records (e.g., Addresses)
#   - Each B line belongs to the most recent A line
#
# EXAMPLE INPUT FILE:
#   ACUST001John Smith           <- Parent (customer_id=CUST001, name=John Smith)
#   B123 Main Street   Anytown    NY12345  <- Child (address for CUST001)
#   B456 Work Avenue   Business   NY10001  <- Child (another address for CUST001)
#   ACUST002Jane Doe             <- Parent (customer_id=CUST002)
#   B789 Oak Lane      Suburb     CA90210  <- Child (address for CUST002)
#   ACUST003Bob Wilson           <- Parent
#   B111 Pine Street   Downtown   TX75001  <- Child
#   B222 Elm Road      Uptown     TX75002  <- Child
#   B333 Maple Drive   Midtown    TX75003  <- Child
#
# OUTPUT (Flattened - one row per child with parent columns repeated):
#   | customer_id | name        | street           | city       | state | zip   |
#   |-------------|-------------|------------------|------------|-------|-------|
#   | CUST001     | John Smith  | 123 Main Street  | Anytown    | NY    | 12345 |
#   | CUST001     | John Smith  | 456 Work Avenue  | Business   | NY    | 10001 |
#   | CUST002     | Jane Doe    | 789 Oak Lane     | Suburb     | CA    | 90210 |
#   | CUST003     | Bob Wilson  | 111 Pine Street  | Downtown   | TX    | 75001 |
#   | CUST003     | Bob Wilson  | 222 Elm Road     | Uptown     | TX    | 75002 |
#   | CUST003     | Bob Wilson  | 333 Maple Drive  | Midtown    | TX    | 75003 |
#
# OUTPUT FILES CREATED:
#   Bronze: s3://bronze-bucket/system=legacy/entity=customer_addresses/dt=2025-01-15/
#     - data.parquet       (flattened parent+child data)
#     - _metadata.json     (schema, source info, lineage)
#     - _checksums.json    (SHA256 hashes for data integrity)
#
# ============================================================================
# BEFORE YOU RUN - Set these 3 environment variables:
# ============================================================================
#
#   Windows (Command Prompt):
#     set S3_ENDPOINT_URL=https://your-minio-server:9000
#     set AWS_ACCESS_KEY_ID=your-access-key
#     set AWS_SECRET_ACCESS_KEY=your-secret-key
#
#   Windows (PowerShell):
#     $env:S3_ENDPOINT_URL = "https://your-minio-server:9000"
#     $env:AWS_ACCESS_KEY_ID = "your-access-key"
#     $env:AWS_SECRET_ACCESS_KEY = "your-secret-key"
#
#   Linux/Mac:
#     export S3_ENDPOINT_URL=https://your-minio-server:9000
#     export AWS_ACCESS_KEY_ID=your-access-key
#     export AWS_SECRET_ACCESS_KEY=your-secret-key
#
# ============================================================================
# TO RUN:
# ============================================================================
#
#   python -m pipelines ./pipelines/examples/work3.yaml --date 2025-01-15
#
#   Options:
#     --dry-run    Validate configuration without writing data
#     --check      Pre-flight check (verify source exists, S3 is reachable)
#     --explain    Show what the pipeline will do without running it
#
# ============================================================================

# Optional: Give your pipeline a name and description
name: parent_child_fixed_width_to_s3
description: Load parent-child fixed-width files (ABABBB pattern) to S3-compatible storage

# ============================================================================
# BRONZE LAYER - Raw Data Extraction
# ============================================================================
# The Bronze layer reads the multi-record-type file and flattens
# parent-child relationships into a single table.

bronze:
  # SYSTEM: Logical grouping for your data source
  #   Examples: "legacy", "mainframe", "as400", "cobol_system"
  system: legacy

  # ENTITY: What this data represents (like a table name)
  #   Examples: "customer_addresses", "orders_items", "invoice_details"
  entity: customer_addresses

  # SOURCE_TYPE: How to read the input file
  #   Use "file_fixed_width" for fixed-width positional files
  source_type: file_fixed_width

  # SOURCE_PATH: Where to read data from
  #   Placeholders: {run_date}, {entity}, {system}
  source_path: "./pipelines/examples/data/{entity}_{run_date}.txt"

  # TARGET_PATH: Where to write Bronze output (S3 bucket)
  target_path: "s3://bronze-bucket/system={system}/entity={entity}/dt={run_date}/"

  # LOAD_PATTERN: How to load data from source
  #   full_snapshot - Load entire dataset each run (safest for fixed-width files)
  load_pattern: full_snapshot

  # ============================================================================
  # MULTI-RECORD TYPE CONFIGURATION (Parent-Child Pattern)
  # ============================================================================
  # This is the key feature! Define how to detect and parse different line types.

  options:
    # RECORD_TYPE_POSITION: Which characters identify the record type?
    #   [0, 1] means: use character 0 (first character) as the type indicator
    #   [0, 2] would mean: use characters 0-1 (first two characters)
    record_type_position: [0, 1]

    # RECORD_TYPES: Define how to handle each type of line
    record_types:
      # TYPE "A" = Parent/Master records (Customers)
      - type: "A"
        role: parent              # This is the parent/master record
        columns:                  # Column names for this record type
          - customer_id           # Characters 1-8 (after the 'A')
          - name                  # Characters 9-28
        widths: [7, 20]           # 7 chars for customer_id, 20 for name

      # TYPE "B" = Child/Detail records (Addresses)
      - type: "B"
        role: child               # These belong to the preceding parent
        columns:                  # Column names for this record type
          - street                # Characters 1-15 (after the 'B')
          - city                  # Characters 16-26
          - state                 # Characters 27-28
          - zip                   # Characters 29-33
        widths: [15, 11, 2, 5]    # 15+11+2+5 = 33 chars total

    # OUTPUT_MODE: How to structure the output
    #   flatten      - Each row = parent columns + child columns (RECOMMENDED)
    #   parent_only  - Extract only parent records (ignore children)
    #   child_only   - Extract only child records (no parent columns)
    output_mode: flatten

    # S3 CREDENTIALS (from environment variables)
    key: ${AWS_ACCESS_KEY_ID}
    secret: ${AWS_SECRET_ACCESS_KEY}

  # ============================================================================
  # S3-COMPATIBLE STORAGE CONFIGURATION
  # ============================================================================
  # For MinIO, Nutanix Objects, or other S3-compatible storage

  s3_endpoint_url: ${S3_ENDPOINT_URL}
  s3_signature_version: s3v4
  s3_addressing_style: path
  s3_verify_ssl: false            # false = allow self-signed certificates

# ============================================================================
# SILVER LAYER - Data Curation
# ============================================================================
# The Silver layer transforms the flattened Bronze data into clean format.
#
# Using periodic_snapshot because:
#   - Full snapshots from Bronze replace all data each run
#   - No deduplication needed (source handles that)
#   - No change history tracking required
#   - Simplest model - just pass through the data

silver:
  # DOMAIN: Business domain this data belongs to
  domain: customers

  # SUBJECT: Subject area within the domain
  subject: addresses

  # MODEL: periodic_snapshot - Simple replacement, no deduplication
  #   Other options:
  #   - full_merge_dedupe: Deduplicate by natural_keys (SCD Type 1)
  #   - scd_type_2: Track full history with effective dates
  #   - event_log: Immutable append-only stream
  model: periodic_snapshot

  # SOURCE_PATH: Where to read Bronze data from
  source_path: "s3://bronze-bucket/system=legacy/entity=customer_addresses/dt={run_date}/"

  # TARGET_PATH: Where to write Silver output
  target_path: "s3://silver-bucket/domain=customers/subject=addresses/"

  # VALIDATE_SOURCE: Skip Bronze checksum validation for speed
  validate_source: skip

  # ============================================================================
  # SILVER S3 CONFIGURATION (Self-contained for independent execution)
  # ============================================================================

  s3_endpoint_url: ${S3_ENDPOINT_URL}
  s3_signature_version: s3v4
  s3_addressing_style: path
  s3_verify_ssl: false
