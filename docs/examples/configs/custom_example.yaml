# CUSTOM EXTRACTOR EXAMPLE
# This demonstrates using a custom Python extractor for specialized sources

platform:
  bronze:
    # Storage backend: "s3", "azure", or "local"
    storage_backend: "s3"

    s3_bucket: "analytics-bronze"
    s3_prefix: "bronze"
    partitioning:
      use_dt_partition: true
    output_defaults:
      allow_csv: true
      allow_parquet: true
      parquet_compression: "snappy"

  s3_connection:
    endpoint_url_env: "BRONZE_S3_ENDPOINT"
    access_key_env: "BRONZE_S3_ACCESS_KEY"
    secret_key_env: "BRONZE_S3_SECRET_KEY"

source:
  type: "custom"
  system: "salesforce"
  table: "accounts"

  custom_extractor:
    module: "examples.custom_extractors.salesforce_example"
    class_name: "SalesforceExampleExtractor"

  salesforce:
    # Your extractor can read this section as needed
    soql_query: "SELECT Id, Name, LastModifiedDate FROM Account"
    # Add incremental or field filters as needed (e.g., cursor fields, event types)

  run:
    max_rows_per_file: 500000      # large chunk size for custom runs
    write_csv: true                # ease of debugging
    write_parquet: true            # analytics format
    s3_enabled: false              # keep local while experimenting
    local_output_dir: "./output"
