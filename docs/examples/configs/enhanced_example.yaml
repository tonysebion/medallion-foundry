# PRODUCTION-READY EXAMPLE WITH ADVANCED FEATURES
# This demonstrates:
# - File size optimization for analytics platforms
# - Hourly partitioning for multiple daily loads
# - Parallel chunk processing
# - Batch metadata tracking

platform:
  bronze:
    # Storage backend: "s3" (AWS/MinIO), "azure" (Blob/ADLS), or "local"
    storage_backend: "s3"

    s3_bucket: "analytics-bronze"
    s3_prefix: "bronze"

    partitioning:
      # Enable date partitioning
      use_dt_partition: true

      # Partition strategy for multiple daily loads
      # Options: "date" (default), "hourly", "timestamp", "batch_id"
      # - "date": dt=YYYY-MM-DD (one partition per day)
      # - "hourly": dt=YYYY-MM-DD/hour=HH (one partition per hour, ideal for multiple daily loads)
      # - "timestamp": dt=YYYY-MM-DD/batch=YYYYMMDD_HHMM (minute-level granularity)
      # - "batch_id": dt=YYYY-MM-DD/batch_id=<custom_or_auto> (custom batch tracking)
      partition_strategy: "hourly"

    output_defaults:
      allow_csv: true
      allow_parquet: true
      parquet_compression: "snappy"  # Options: snappy, gzip, brotli, lz4, zstd

  s3_connection:
    endpoint_url_env: "BRONZE_S3_ENDPOINT"
    access_key_env: "BRONZE_S3_ACCESS_KEY"
    secret_key_env: "BRONZE_S3_SECRET_KEY"

source:
  type: "api"
  system: "salesforce"
  table: "accounts"

  api:
    base_url: "https://api.salesforce.com"
    endpoint: "/services/data/v58.0/query"
    method: "GET"

    auth_type: "bearer"
    auth_token_env: "SALESFORCE_TOKEN"

    params:
      q: "SELECT Id, Name, Industry, CreatedDate FROM Account WHERE IsDeleted = false"

    pagination:
      type: "cursor"
      next_token_path: "nextRecordsUrl"
      records_path: "records"

  run:
    # File size controls - optimize for analytics query engines
    # Analytics platforms perform best with files between 128MB-1GB
    max_rows_per_file: 1000000  # Maximum rows per file (0 = unlimited)
    max_file_size_mb: 256       # Maximum file size in MB (optional, overrides row limit)

    # Parallel processing within this extraction job
    # Processes multiple chunks concurrently for faster extraction
    parallel_workers: 4         # Number of parallel threads for chunk processing (default: 1)

    # Output formats
    write_csv: false            # Disable CSV for production (Parquet is more efficient)
    write_parquet: true         # Enable Parquet for optimal analytics performance

    # S3 upload
    s3_enabled: true

    # Local staging directory
    local_output_dir: "./output/bronze"

    # Cleanup failed extractions
    cleanup_on_failure: true

    # Optional: Custom batch_id for partition_strategy="batch_id"
    # If not provided, auto-generated as YYYYMMDD_HHMMSS_<random>
    # batch_id: "batch_20250112_001"
