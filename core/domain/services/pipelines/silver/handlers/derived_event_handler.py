"""Handler for DERIVED_EVENT entity type processing."""

from __future__ import annotations

from typing import Any, Dict, List, Optional, cast

import pandas as pd

from core.infrastructure.config import DeleteMode, EntityKind
from core.domain.services.pipelines.silver.handlers.base import BasePatternHandler
from core.domain.services.pipelines.silver.handlers.registry import register_handler


@register_handler(EntityKind.DERIVED_EVENT)
class DerivedEventHandler(BasePatternHandler):
    """Handler for DERIVED_EVENT entity type.

    Derived events are change events generated by comparing consecutive state versions.
    This handler:
    - Groups data by natural keys
    - Compares each row to its predecessor
    - Generates change events with change_type (upsert, update, delete, noop)
    """

    def validate(self, df: pd.DataFrame) -> None:
        """Validate DataFrame has required columns for derived event generation.

        Args:
            df: Input DataFrame to validate.

        Raises:
            ValueError: If timestamp column is not configured or missing from data.
        """
        ts_col = self.change_ts_column or self.event_ts_column
        if not ts_col:
            raise ValueError(
                "change_ts_column (or event_ts_column) required for derived_event datasets"
            )
        if ts_col not in df.columns:
            raise ValueError(f"Timestamp column '{ts_col}' not found in data")

    def process(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """Generate derived events from state changes.

        Args:
            df: Input DataFrame with state data.

        Returns:
            Dict with single "derived_events" key containing change event DataFrame.
        """
        self.validate(df)

        ts_col = self.change_ts_column or self.event_ts_column
        assert ts_col is not None  # validate() ensures this

        attrs = self.dataset.silver.attributes or []
        rows: List[Dict[str, Any]] = []

        grouped = df.sort_values(self.natural_keys + [ts_col]).groupby(
            self.natural_keys
        )

        for _, group in grouped:
            prev: Optional[pd.Series] = None
            for _, row_raw in group.iterrows():
                row = cast(pd.Series, row_raw)
                change_type = "upsert"
                changed_cols = list(attrs)

                if prev is not None:
                    changed_cols = [
                        col for col in attrs if row.get(col) != prev.get(col)
                    ]
                    if (
                        not changed_cols
                        and self.dataset.silver.delete_mode == DeleteMode.IGNORE
                    ):
                        prev = row
                        continue
                    change_type = "update" if changed_cols else "noop"

                if (
                    self.dataset.silver.delete_mode == DeleteMode.TOMBSTONE_EVENT
                    and row.get("is_deleted")
                ):
                    change_type = "delete"

                event = {key: row[key] for key in self.natural_keys}
                for attr in attrs:
                    event[attr] = row.get(attr)
                event_ts_name = self.event_ts_column or "event_ts"
                event[event_ts_name] = row[ts_col]
                event["change_type"] = change_type
                event["changed_columns"] = ",".join(changed_cols)
                rows.append(event)
                prev = row

        if not rows:
            return {"derived_events": pd.DataFrame(columns=self.natural_keys + attrs)}

        return {"derived_events": pd.DataFrame(rows)}
