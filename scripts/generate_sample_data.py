"""Generate realistic Bronze sample datasets for testing load patterns."""

from __future__ import annotations

import csv
import json
from datetime import datetime, timedelta, timezone
from pathlib import Path
from random import Random
from typing import Iterable, List, Dict

BASE_DIR = Path(__file__).resolve().parents[1] / "docs" / "examples" / "data" / "bronze_samples"


FULL_DATES = ["2025-11-13", "2025-11-14"]
CDC_DATES = ["2025-11-13", "2025-11-14"]
CURRENT_HISTORY_DATES = ["2025-11-13", "2025-11-14"]


def _write_csv(path: Path, rows: Iterable[Dict[str, object]]) -> None:
    rows = list(rows)
    if not rows:
        return
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=rows[0].keys())
        writer.writeheader()
        writer.writerows(rows)


def _write_metadata(
    target_dir: Path,
    pattern: str,
    record_count: int,
    chunk_count: int,
    reference_mode: Dict[str, object] | None = None,
) -> None:
    metadata = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "record_count": record_count,
        "chunk_count": chunk_count,
        "load_pattern": pattern,
        "notes": "Autogenerated sample data for Bronze/Silver integration tests.",
    }
    if reference_mode:
        metadata["reference_mode"] = reference_mode
    with (target_dir / "_metadata.json").open("w", encoding="utf-8") as handle:
        json.dump(metadata, handle, indent=2)


def generate_full_snapshot(seed: int = 42, row_count: int = 500) -> None:
    for day_offset, date_str in enumerate(FULL_DATES):
        rng = Random(seed + day_offset)
        base_dir = BASE_DIR / "full" / "system=retail_demo" / "table=orders" / "pattern=full" / f"dt={date_str}"
        rows: List[Dict[str, object]] = []
        start = datetime.fromisoformat(f"{date_str}T00:00:00")
        total_rows = row_count + day_offset * 50
        for order_id in range(1, total_rows + 1):
            order_time = start + timedelta(hours=order_id)
            rows.append(
                {
                    "order_id": f"ORD-{order_id + day_offset * 1000:05d}",
                    "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                    "status": rng.choice(["new", "processing", "shipped", "delivered", "returned"]),
                    "order_total": round(rng.uniform(25.0, 500.0), 2),
                    "updated_at": order_time.isoformat() + "Z",
                    "run_date": date_str,
                }
            )
        rows.append(
            {
                "order_id": None,
                "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                "status": "processing",
                "order_total": round(rng.uniform(25.0, 500.0), 2),
                "updated_at": (start + timedelta(days=total_rows + 1)).isoformat() + "Z",
                "run_date": date_str,
            }
        )

        chunk_path = base_dir / "full-part-0001.csv"
        _write_csv(chunk_path, rows)
        total_records = len(rows)
        chunk_count = 1

        if day_offset == 1:
            schema_rows: List[Dict[str, object]] = []
            for idx in range(30):
                schema_rows.append(
                    {
                        "order_id": f"ORD-SC-{idx:05d}",
                        "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                        "status": rng.choice(["high-priority", "standard"]),
                        "order_total": round(rng.uniform(10.0, 999.0), 2),
                        "updated_at": (start + timedelta(hours=idx)).isoformat() + "Z",
                        "run_date": date_str,
                        "campaign_id": f"CAM-{rng.randint(100, 999)}",
                        "priority": rng.choice(["high", "normal", "low"]),
                    }
                )
            schema_chunk = base_dir / "full-part-0002.csv"
            _write_csv(schema_chunk, schema_rows)
            total_records += len(schema_rows)
            chunk_count += 1

        _write_metadata(base_dir, "full", total_records, chunk_count)


def _write_hybrid_reference(base_dir: Path, date_str: str, seed: int, role: str, delta_patterns: List[str]) -> None:
    rng = Random(seed)
    rows: List[Dict[str, object]] = []
    start = datetime.fromisoformat(f"{date_str}T00:00:00")
    for order_id in range(1, 151):
        rows.append(
            {
                "order_id": f"HYB-{order_id:05d}",
                "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                "status": rng.choice(["new", "shipped"]),
                "order_total": round(rng.uniform(10.0, 250.0), 2),
                "updated_at": (start + timedelta(hours=order_id)).isoformat() + "Z",
                "run_date": date_str,
            }
        )
    chunk_path = base_dir / "reference-part-0001.csv"
    _write_csv(chunk_path, rows)
    _write_metadata(
        base_dir,
        "full",
        len(rows),
        1,
        reference_mode={"role": "reference", "cadence_days": 7, "delta_patterns": delta_patterns},
    )


def _write_hybrid_delta(base_dir: Path, date_str: str, delta_pattern: str, seed: int, role: str) -> None:
    rng = Random(seed)
    rows: List[Dict[str, object]] = []
    start = datetime.fromisoformat(f"{date_str}T08:00:00")
    for idx in range(1, 51):
        rows.append(
            {
                "order_id": f"HYB-{idx + 150:05d}",
                "change_type": rng.choice(["insert", "update"]),
                "changed_at": (start + timedelta(minutes=idx * 5)).isoformat() + "Z",
                "order_total": round(rng.uniform(15.0, 300.0), 2),
                "run_date": date_str,
            }
        )
    chunk_path = base_dir / "delta-part-0001.csv"
    _write_csv(chunk_path, rows)
    _write_metadata(
        base_dir,
        delta_pattern,
        len(rows),
        1,
        reference_mode={"role": role, "delta_patterns": [delta_pattern]},
    )


def generate_cdc(seed: int = 99, row_count: int = 400) -> None:
    change_types = ["insert", "update", "delete"]
    for day_offset, date_str in enumerate(CDC_DATES):
        rng = Random(seed + day_offset)
        base_dir = BASE_DIR / "cdc" / "system=retail_demo" / "table=orders" / "pattern=cdc" / f"dt={date_str}"
        rows: List[Dict[str, object]] = []
        start = datetime.fromisoformat(f"{date_str}T08:00:00")

        total_rows = row_count + day_offset * 60
        for idx in range(1, total_rows + 1):
            change_time = start + timedelta(minutes=idx * 3)
            rows.append(
                {
                    "order_id": f"ORD-{rng.randint(1, 800):05d}",
                    "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                    "change_type": rng.choice(change_types),
                    "changed_at": change_time.isoformat() + "Z",
                    "status": rng.choice(["processing", "shipped", "cancelled"]),
                    "order_total": round(rng.uniform(10.0, 800.0), 2),
                    "run_date": date_str,
                }
            )

        rows.append(
            {
                "order_id": None,
                "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                "change_type": "insert",
                "changed_at": (start + timedelta(minutes=total_rows * 3 + 5)).isoformat() + "Z",
                "status": "processing",
                "order_total": round(rng.uniform(10.0, 800.0), 2),
                "run_date": date_str,
            }
        )

        chunk_path = base_dir / "cdc-part-0001.csv"
        _write_csv(chunk_path, rows)
        total_records = len(rows)
        chunk_count = 1

        if day_offset == 1:
            schema_rows: List[Dict[str, object]] = []
            for idx in range(20):
                note_time = start + timedelta(minutes=(idx + 1) * 2)
                schema_rows.append(
                    {
                        "order_id": f"ORD-{rng.randint(1, 1200):05d}",
                        "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                        "change_type": rng.choice(["update", "insert"]),
                        "changed_at": note_time.isoformat() + "Z",
                        "status": "delta",
                        "order_total": round(rng.uniform(5.0, 1200.0), 2),
                        "run_date": date_str,
                        "note": f"schema-change-{idx}",
                    }
                )
            schema_chunk = base_dir / "cdc-part-0002.csv"
            _write_csv(schema_chunk, schema_rows)
            total_records += len(schema_rows)
            chunk_count += 1

        _write_metadata(base_dir, "cdc", total_records, chunk_count)


def generate_current_history(seed: int = 7, current_rows: int = 200, history_rows: int = 600) -> None:
    for day_offset, date_str in enumerate(CURRENT_HISTORY_DATES):
        rng = Random(seed + day_offset)
        base_dir = (
            BASE_DIR
            / "current_history"
            / "system=retail_demo"
            / "table=orders"
            / "pattern=current_history"
            / f"dt={date_str}"
        )

        def build_history_rows() -> List[Dict[str, object]]:
            rows: List[Dict[str, object]] = []
            base_time = datetime(2024, 1, 1) + timedelta(days=day_offset * 30)
            total_rows = history_rows + day_offset * 80
            for idx in range(1, total_rows + 1):
                start_ts = base_time + timedelta(days=rng.randint(0, 365))
                end_ts = start_ts + timedelta(days=rng.randint(5, 120))
                rows.append(
                    {
                        "order_id": f"ORD-{rng.randint(1, 1200):05d}",
                        "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                        "status": rng.choice(["active", "expired", "suspended"]),
                        "effective_start": start_ts.isoformat() + "Z",
                        "effective_end": end_ts.isoformat() + "Z",
                        "current_flag": 0,
                        "updated_at": (end_ts + timedelta(hours=2)).isoformat() + "Z",
                        "run_date": date_str,
                    }
                )
            return rows

        def build_current_rows() -> List[Dict[str, object]]:
            rows: List[Dict[str, object]] = []
            start_time = datetime.fromisoformat(f"{date_str}T00:00:00")
            total_rows = current_rows + day_offset * 40
            for idx in range(1, total_rows + 1):
                rows.append(
                    {
                        "order_id": f"ORD-{idx + 900 + day_offset * 500:05d}",
                        "customer_id": f"CUST-{rng.randint(2000, 9999)}",
                        "status": rng.choice(["active", "pending", "suspended"]),
                        "effective_start": "",
                        "effective_end": "",
                        "current_flag": 1,
                        "updated_at": (start_time + timedelta(hours=idx)).isoformat() + "Z",
                        "run_date": date_str,
                    }
                )
            return rows

        history_data = build_history_rows()
        current_data = build_current_rows()
        combined_rows = history_data + current_data
        combined_rows.append(
            {
                "order_id": None,
                "customer_id": "",
                "status": "unknown",
                "effective_start": "",
                "effective_end": "",
                "current_flag": "",
                "updated_at": datetime.fromisoformat(f"{date_str}T00:00:00").isoformat() + "Z",
                "run_date": date_str,
            }
        )
        total_records = len(combined_rows)
        chunk_count = 1
        _write_csv(base_dir / "current-history-part-0001.csv", combined_rows)

        if day_offset == 1:
            skew_rows: List[Dict[str, object]] = []
            for idx in range(150):
                skew_rows.append(
                    {
                        "order_id": f"ORD-SK-{idx:05d}",
                        "customer_id": f"CUST-{rng.randint(2000, 9999)}",
                        "status": "active",
                        "effective_start": (datetime.fromisoformat(f"{date_str}T00:00:00") - timedelta(days=idx % 5)).isoformat() + "Z",
                        "effective_end": "",
                        "current_flag": 1,
                        "updated_at": (datetime.fromisoformat(f"{date_str}T12:00:00") + timedelta(minutes=idx)).isoformat() + "Z",
                        "run_date": date_str,
                        "revision_notes": f"skewed-{idx % 3}",
                    }
                )
            skew_chunk = base_dir / "current-history-part-0002.csv"
            _write_csv(skew_chunk, skew_rows)
            total_records += len(skew_rows)
            chunk_count += 1

        _write_metadata(base_dir, "current_history", total_records, chunk_count)


def generate_hybrid_combinations(seed: int = 123) -> None:
    combos = [
        ("hybrid_cdc", "cdc"),
        ("hybrid_incremental", "incremental_merge"),
    ]
    for combo_name, delta_pattern in combos:
        for day_offset, date_str in enumerate(FULL_DATES):
            base_dir = (
                BASE_DIR
                / combo_name
                / "system=retail_demo"
                / "table=orders"
                / f"pattern={combo_name}"
                / f"dt={date_str}"
            )
            reference_dir = base_dir / "reference"
            delta_dir = base_dir / "delta"

            _write_hybrid_reference(
                reference_dir,
                date_str,
                seed + day_offset,
                "reference",
                [delta_pattern],
            )

            _write_hybrid_delta(
                delta_dir,
                date_str,
                delta_pattern,
                seed + day_offset + 500,
                "delta",
            )


def main() -> None:
    generate_full_snapshot()
    generate_cdc()
    generate_current_history()
    generate_hybrid_combinations()
    print(f"Sample datasets written under {BASE_DIR}")


if __name__ == "__main__":
    main()
