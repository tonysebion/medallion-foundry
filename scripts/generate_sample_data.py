"""Generate realistic Bronze sample datasets for testing load patterns."""

from __future__ import annotations

import csv
import json
from datetime import datetime, timedelta, timezone
from pathlib import Path
from random import Random
from typing import Iterable, List, Dict

BASE_DIR = Path(__file__).resolve().parents[1] / "docs" / "examples" / "data" / "bronze_samples"


FULL_DATES = ["2025-11-13", "2025-11-14"]
CDC_DATES = ["2025-11-13", "2025-11-14"]
CURRENT_HISTORY_DATES = ["2025-11-13", "2025-11-14"]
HYBRID_REFERENCE_INITIAL = datetime(2025, 11, 13).date()
HYBRID_REFERENCE_SECOND = datetime(2025, 11, 21).date()
HYBRID_DELTA_DAYS = 14
HYBRID_REFERENCE_SWITCH_DAY = 8
HYBRID_COMBOS = [
    ("hybrid_cdc_point", "cdc", "point_in_time"),
    ("hybrid_cdc_cumulative", "cdc", "cumulative"),
    ("hybrid_incremental_point", "incremental_merge", "point_in_time"),
    ("hybrid_incremental_cumulative", "incremental_merge", "cumulative"),
]


def _write_csv(path: Path, rows: Iterable[Dict[str, object]]) -> None:
    rows = list(rows)
    if not rows:
        return
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=rows[0].keys())
        writer.writeheader()
        writer.writerows(rows)


def _write_metadata(
    target_dir: Path,
    pattern: str,
    record_count: int,
    chunk_count: int,
    reference_mode: Dict[str, object] | None = None,
) -> None:
    metadata = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "record_count": record_count,
        "chunk_count": chunk_count,
        "load_pattern": pattern,
        "notes": "Autogenerated sample data for Bronze/Silver integration tests.",
    }
    if reference_mode:
        metadata["reference_mode"] = reference_mode
    with (target_dir / "_metadata.json").open("w", encoding="utf-8") as handle:
        json.dump(metadata, handle, indent=2)


def generate_full_snapshot(seed: int = 42, row_count: int = 500) -> None:
    for day_offset, date_str in enumerate(FULL_DATES):
        rng = Random(seed + day_offset)
        base_dir = BASE_DIR / "full" / "system=retail_demo" / "table=orders" / "pattern=full" / f"dt={date_str}"
        rows: List[Dict[str, object]] = []
        start = datetime.fromisoformat(f"{date_str}T00:00:00")
        total_rows = row_count + day_offset * 50
        for order_id in range(1, total_rows + 1):
            order_time = start + timedelta(hours=order_id)
            rows.append(
                {
                    "order_id": f"ORD-{order_id + day_offset * 1000:05d}",
                    "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                    "status": rng.choice(["new", "processing", "shipped", "delivered", "returned"]),
                    "order_total": round(rng.uniform(25.0, 500.0), 2),
                    "updated_at": order_time.isoformat() + "Z",
                    "run_date": date_str,
                }
            )
        rows.append(
            {
                "order_id": None,
                "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                "status": "processing",
                "order_total": round(rng.uniform(25.0, 500.0), 2),
                "updated_at": (start + timedelta(days=total_rows + 1)).isoformat() + "Z",
                "run_date": date_str,
            }
        )

        chunk_path = base_dir / "full-part-0001.csv"
        _write_csv(chunk_path, rows)
        total_records = len(rows)
        chunk_count = 1

        if day_offset == 1:
            schema_rows: List[Dict[str, object]] = []
            for idx in range(30):
                schema_rows.append(
                    {
                        "order_id": f"ORD-SC-{idx:05d}",
                        "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                        "status": rng.choice(["high-priority", "standard"]),
                        "order_total": round(rng.uniform(10.0, 999.0), 2),
                        "updated_at": (start + timedelta(hours=idx)).isoformat() + "Z",
                        "run_date": date_str,
                        "campaign_id": f"CAM-{rng.randint(100, 999)}",
                        "priority": rng.choice(["high", "normal", "low"]),
                    }
                )
            schema_chunk = base_dir / "full-part-0002.csv"
            _write_csv(schema_chunk, schema_rows)
            total_records += len(schema_rows)
            chunk_count += 1

        _write_metadata(base_dir, "full", total_records, chunk_count)


def _write_hybrid_reference(
    base_dir: Path, date_str: str, seed: int, delta_patterns: List[str], delta_mode: str
) -> None:
    rng = Random(seed)
    rows: List[Dict[str, object]] = []
    start = datetime.fromisoformat(f"{date_str}T00:00:00")
    for order_id in range(1, 151):
        rows.append(
            {
                "order_id": f"HYB-{order_id:05d}",
                "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                "status": rng.choice(["new", "shipped"]),
                "order_total": round(rng.uniform(10.0, 250.0), 2),
                "updated_at": (start + timedelta(hours=order_id)).isoformat() + "Z",
                "run_date": date_str,
            }
        )
    chunk_path = base_dir / "reference-part-0001.csv"
    _write_csv(chunk_path, rows)
    _write_metadata(
        base_dir,
        "full",
        len(rows),
        1,
        reference_mode={
            "role": "reference",
            "cadence_days": 7,
            "delta_patterns": delta_patterns,
        "reference_run_date": date_str,
        "delta_mode": delta_mode,
    },
)


def _write_hybrid_delta(
    base_dir: Path,
    date_str: str,
    delta_pattern: str,
    seed: int,
    role: str,
    rows: List[Dict[str, object]],
    delta_mode: str,
    reference_run_date: dt.date,
) -> None:
    base_dir.mkdir(parents=True, exist_ok=True)
    rng = Random(seed)
    chunk_path = base_dir / "delta-part-0001.csv"
    _write_csv(chunk_path, rows)
    _write_metadata(
        base_dir,
        delta_pattern,
        len(rows),
        1,
        reference_mode={
            "role": role,
            "delta_patterns": [delta_pattern],
            "reference_run_date": reference_run_date.isoformat(),
            "delta_mode": delta_mode,
        },
    )


def _build_delta_rows(delta_pattern: str, date_str: str, seed: int) -> List[Dict[str, object]]:
    rng = Random(seed)
    start = datetime.fromisoformat(f"{date_str}T08:00:00")
    rows: List[Dict[str, object]] = []
    for idx in range(1, 51):
        rows.append(
            {
                "order_id": f"HYB-{idx + 150:05d}",
                "change_type": rng.choice(["insert", "update"]),
                "changed_at": (start + timedelta(minutes=idx * 5)).isoformat() + "Z",
                "order_total": round(rng.uniform(15.0, 300.0), 2),
                "delta_tag": f"{delta_pattern}-{date_str}",
                "run_date": date_str,
            }
        )
    return rows


def generate_cdc(seed: int = 99, row_count: int = 400) -> None:
    change_types = ["insert", "update", "delete"]
    for day_offset, date_str in enumerate(CDC_DATES):
        rng = Random(seed + day_offset)
        base_dir = BASE_DIR / "cdc" / "system=retail_demo" / "table=orders" / "pattern=cdc" / f"dt={date_str}"
        rows: List[Dict[str, object]] = []
        start = datetime.fromisoformat(f"{date_str}T08:00:00")

        total_rows = row_count + day_offset * 60
        for idx in range(1, total_rows + 1):
            change_time = start + timedelta(minutes=idx * 3)
            rows.append(
                {
                    "order_id": f"ORD-{rng.randint(1, 800):05d}",
                    "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                    "change_type": rng.choice(change_types),
                    "changed_at": change_time.isoformat() + "Z",
                    "status": rng.choice(["processing", "shipped", "cancelled"]),
                    "order_total": round(rng.uniform(10.0, 800.0), 2),
                    "run_date": date_str,
                }
            )

        rows.append(
            {
                "order_id": None,
                "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                "change_type": "insert",
                "changed_at": (start + timedelta(minutes=total_rows * 3 + 5)).isoformat() + "Z",
                "status": "processing",
                "order_total": round(rng.uniform(10.0, 800.0), 2),
                "run_date": date_str,
            }
        )

        chunk_path = base_dir / "cdc-part-0001.csv"
        _write_csv(chunk_path, rows)
        total_records = len(rows)
        chunk_count = 1

        if day_offset == 1:
            schema_rows: List[Dict[str, object]] = []
            for idx in range(20):
                note_time = start + timedelta(minutes=(idx + 1) * 2)
                schema_rows.append(
                    {
                        "order_id": f"ORD-{rng.randint(1, 1200):05d}",
                        "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                        "change_type": rng.choice(["update", "insert"]),
                        "changed_at": note_time.isoformat() + "Z",
                        "status": "delta",
                        "order_total": round(rng.uniform(5.0, 1200.0), 2),
                        "run_date": date_str,
                        "note": f"schema-change-{idx}",
                    }
                )
            schema_chunk = base_dir / "cdc-part-0002.csv"
            _write_csv(schema_chunk, schema_rows)
            total_records += len(schema_rows)
            chunk_count += 1

        _write_metadata(base_dir, "cdc", total_records, chunk_count)


def generate_current_history(seed: int = 7, current_rows: int = 200, history_rows: int = 600) -> None:
    for day_offset, date_str in enumerate(CURRENT_HISTORY_DATES):
        rng = Random(seed + day_offset)
        base_dir = (
            BASE_DIR
            / "current_history"
            / "system=retail_demo"
            / "table=orders"
            / "pattern=current_history"
            / f"dt={date_str}"
        )

        def build_history_rows() -> List[Dict[str, object]]:
            rows: List[Dict[str, object]] = []
            base_time = datetime(2024, 1, 1) + timedelta(days=day_offset * 30)
            total_rows = history_rows + day_offset * 80
            for idx in range(1, total_rows + 1):
                start_ts = base_time + timedelta(days=rng.randint(0, 365))
                end_ts = start_ts + timedelta(days=rng.randint(5, 120))
                rows.append(
                    {
                        "order_id": f"ORD-{rng.randint(1, 1200):05d}",
                        "customer_id": f"CUST-{rng.randint(1000, 9999)}",
                        "status": rng.choice(["active", "expired", "suspended"]),
                        "effective_start": start_ts.isoformat() + "Z",
                        "effective_end": end_ts.isoformat() + "Z",
                        "current_flag": 0,
                        "updated_at": (end_ts + timedelta(hours=2)).isoformat() + "Z",
                        "run_date": date_str,
                    }
                )
            return rows

        def build_current_rows() -> List[Dict[str, object]]:
            rows: List[Dict[str, object]] = []
            start_time = datetime.fromisoformat(f"{date_str}T00:00:00")
            total_rows = current_rows + day_offset * 40
            for idx in range(1, total_rows + 1):
                rows.append(
                    {
                        "order_id": f"ORD-{idx + 900 + day_offset * 500:05d}",
                        "customer_id": f"CUST-{rng.randint(2000, 9999)}",
                        "status": rng.choice(["active", "pending", "suspended"]),
                        "effective_start": "",
                        "effective_end": "",
                        "current_flag": 1,
                        "updated_at": (start_time + timedelta(hours=idx)).isoformat() + "Z",
                        "run_date": date_str,
                    }
                )
            return rows

        history_data = build_history_rows()
        current_data = build_current_rows()
        combined_rows = history_data + current_data
        combined_rows.append(
            {
                "order_id": None,
                "customer_id": "",
                "status": "unknown",
                "effective_start": "",
                "effective_end": "",
                "current_flag": "",
                "updated_at": datetime.fromisoformat(f"{date_str}T00:00:00").isoformat() + "Z",
                "run_date": date_str,
            }
        )
        total_records = len(combined_rows)
        chunk_count = 1
        _write_csv(base_dir / "current-history-part-0001.csv", combined_rows)

        if day_offset == 1:
            skew_rows: List[Dict[str, object]] = []
            for idx in range(150):
                skew_rows.append(
                    {
                        "order_id": f"ORD-SK-{idx:05d}",
                        "customer_id": f"CUST-{rng.randint(2000, 9999)}",
                        "status": "active",
                        "effective_start": (datetime.fromisoformat(f"{date_str}T00:00:00") - timedelta(days=idx % 5)).isoformat() + "Z",
                        "effective_end": "",
                        "current_flag": 1,
                        "updated_at": (datetime.fromisoformat(f"{date_str}T12:00:00") + timedelta(minutes=idx)).isoformat() + "Z",
                        "run_date": date_str,
                        "revision_notes": f"skewed-{idx % 3}",
                    }
                )
            skew_chunk = base_dir / "current-history-part-0002.csv"
            _write_csv(skew_chunk, skew_rows)
            total_records += len(skew_rows)
            chunk_count += 1

        _write_metadata(base_dir, "current_history", total_records, chunk_count)


def _build_delta_rows(delta_pattern: str, date_str: str, seed: int) -> List[Dict[str, object]]:
    rng = Random(seed)
    rows: List[Dict[str, object]] = []
    start = datetime.fromisoformat(f"{date_str}T08:00:00")
    for idx in range(1, 51):
        rows.append(
            {
                "order_id": f"HYB-{idx + 150:05d}",
                "change_type": rng.choice(["insert", "update"]),
                "changed_at": (start + timedelta(minutes=idx * 5)).isoformat() + "Z",
                "order_total": round(rng.uniform(15.0, 300.0), 2),
                "delta_tag": f"{delta_pattern}-{date_str}",
                "run_date": date_str,
            }
        )
    return rows


def generate_hybrid_combinations(seed: int = 123) -> None:
    for combo_name, delta_pattern, delta_mode in HYBRID_COMBOS:
        base_pattern_dir = (
            BASE_DIR
            / combo_name
            / "system=retail_demo"
            / "table=orders"
            / f"pattern={combo_name}"
        )
        for ref_date in (HYBRID_REFERENCE_INITIAL, HYBRID_REFERENCE_SECOND):
            ref_dir = base_pattern_dir / f"dt={ref_date.isoformat()}" / "reference"
            _write_hybrid_reference(
                ref_dir,
                ref_date.isoformat(),
                seed,
                [delta_pattern],
                delta_mode,
            )
        cumulative_rows: List[Dict[str, object]] = []
        for offset in range(1, HYBRID_DELTA_DAYS + 1):
            delta_date = HYBRID_REFERENCE_INITIAL + timedelta(days=offset)
            rows = _build_delta_rows(delta_pattern, delta_date.isoformat(), seed + offset * 10)
            if delta_mode == "cumulative":
                cumulative_rows.extend(rows)
                rows_to_write = cumulative_rows.copy()
            else:
                rows_to_write = rows
            delta_dir = base_pattern_dir / f"dt={delta_date.isoformat()}" / "delta"
            reference_run_date = (
                HYBRID_REFERENCE_INITIAL if offset < HYBRID_REFERENCE_SWITCH_DAY else HYBRID_REFERENCE_SECOND
            )
            _write_hybrid_delta(
                delta_dir,
                delta_date.isoformat(),
                delta_pattern,
                seed + offset * 10,
                "delta",
                rows_to_write,
                delta_mode,
                reference_run_date,
            )


def main() -> None:
    generate_full_snapshot()
    generate_cdc()
    generate_current_history()
    generate_hybrid_combinations()
    print(f"Sample datasets written under {BASE_DIR}")


if __name__ == "__main__":
    main()
HYBRID_COMBOS = [
    ("hybrid_cdc_point", "cdc", "point_in_time"),
    ("hybrid_cdc_cumulative", "cdc", "cumulative"),
    ("hybrid_incremental_point", "incremental_merge", "point_in_time"),
    ("hybrid_incremental_cumulative", "incremental_merge", "cumulative"),
]
